% status: 0
% chapter: TBD

\title{Paper: Big Data in Psychology}


\author{Qingyun Lin}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{Smith Research Center}
  \city{Bloomington} 
  \state{IN} 
  \postcode{47408}
}
\email{ql10@iu.edu}


% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{G. v. Laszewski}


\begin{abstract}
There is much discussion about the value of company big data. 
For example, Amazon will match your shopping and page views with 
other shoppers’ purchases and pageviews, and try to find people with 
similar interests. Then Amazon recommends buying products that people 
like, assuming you will like them.

Can big data be applied to answer the research community’s interest 
in psychology?

This paper will introduce what is pychology and what is big data 
technique simply, and why these two fields can be combined togethor, 
how can the society benefit from this combination, and introduce 
several combination examples and what is the future of this field.
\end{abstract}

\keywords{hid-sp18-515, big data, pychology, combination, future}


\maketitle

\section{Introduction}

Pychology problem is getting more and more attention in the morden
society. And as the big data technology is such a heated topic 
today, the combination of this two fields should be well applied.
Big data is particularly good at dealing with issues that people 
may be reluctant to answer surveys. Often, the way people interact
 with computers reveals the interest they would not express in 
interviews or anonymous surveys.

Significant advances in computing technology, coupled with the 
proliferation of large digital networks and social media platforms,
 have produced an unimaginable amount of information about people.
It is estimated that the amount of digital data currently present 
is in the thousands of exabytes or 10 to the 18th power of bytes
~\cite{editor00}.

This era of big data is likely to change the way psychologists 
observe human behavior. However, just as it creates new 
opportunities, access to a large amount of information also brings
 new challenges to research~\cite{editor00}. Michael N. Jones of 
Indiana University Bloomington introduced the Big Data Project at 
the 2014 APS Annual Conference.

Each and every piece of data is a trace of human direct and 
offers us a potential snippet of data to comprehension 
fundamental mental gauges, said Jones~\cite{editor00}. 
However, we should have the ability to gather every last one of 
those pieces adequately to better understand the major mental 
tenets that made them.

The study of language development as one of Jones's own research 
interests is a good example of big data research~\cite{editor00}. 
Collecting a large number of infant samples from the natural 
environment is time-consuming and often results in small sample 
sizes. The test theory of the way children learn languages is a 
long time.

Big data can help speed up the process. As a proof of concept, 
Jones showed that based on associative learning theory, more than 
100,000 words from natural language can be entered into a computer
 model - the idea is that children group words together according 
to how often they use them near other words. 
Jones said that as the
 analysis progressed, the model did realize that the relationship 
between computers and data and word classes was closer than We can 
computers and obsolete~\cite{editor00}.

Jones said that in the end, a similar analysis can be used to study
 the theory of connected learning in direct child conversation 
samples. As long as they have enough data to continue, these 
models are very good at learning from noise, he said
~\cite{editor00}.

Tanzeem Choudhury, an information scientist at Cornell University, 
said that big data may help researchers reach the point where they 
can collect behavioral information without sampling human 
participants~\cite{editor00}. Technologies such as smart phones and
 wearable sensors can collect information about physical activity,
 social interaction, geographic location, and more.

The result of this data collection is that it is effective for 
users; it does not require their time or effort, and it greatly 
reduces self-reporting errors.

We can persistently get estimations of conduct without bothering 
individuals to round out reviews, said Choudhury. We can possibly 
get persistent estimation without really engaging clients all 
the time and depend on their self-input~\cite{editor00}.

Chowdhury participated in some of these projects. StressSense can 
track 
daytime stress and help them avoid anxiety. For example, MyBehavior uses 
physical activity patterns to suggest ways of maintaining the shape, for 
example, working more frequently along lines that the user seems to like. 
MoodRhythm allows patients with bipolar disorder to monitor sleep and social 
interactions to maintain emotional and energy balance, which is a major 
improvement in pen and daily behavioral tracking~\cite{editor00}. 

Big data has produced positive changes in the online search field. 
Susan T.Dumais of Microsoft Research Inc said that the billions of 
Internet searches that happen each day leave the behavior log that 
analysts use to improve search engines. Without such a large record, 
sites such as Google and Bing will never be able to use 2.4 words in 
an average Internet search and turn it into useful 
content~\cite{editor00}.

Behavioral logs allow us to characterize, with a richness and fidelity 
that we’ve never had before, what it is people are trying to do with
the tools and systems they’re interacting with, said Dumais~\cite{editor00}.

By mining behavior logs, analysts can create personalized algorithms 
to improve the user's search experience. For example, if Dumais searches 
for sigir, she may need the homepage of the Information Search Special 
Interest Group (SIGIR). If Little Stuart Bowen performs the same 
search, 
he may need the site to hold his position: Special Inspector General 
for Reconstruction of Iraq (also known as SIGIR).

In other words, the system can know that words and acronyms do not always 
predict the best way users want to search. Modeling the search in a way 
that considers the context of the published query is very important for 
improving Web search. Previous search activity is important, and the 
location and time of the query are also important. For example, the 
search for Spring US Open may refer to a golf ball, while the same 
search in late summer may refer to tennis.

Before you could gather Big Data, the individual who talked 
loudest, or then again the most generously compensated individual's
 assessment, would overwhelm, said Dumais. Presently the 
information, particularly when gotten from precisely controlled
 Web-scale tests, overwhelms~\cite{editor00}.Enormous Data likewise
 enables scientists to reconsider past issues in new ways, said 
Brian M. D'Onofrio, an APS specialist at Indiana University 
Bloomington. Specifically, he brought up that specialists ought to
 consider re-modifying information that might be gathered for 
different reasons. Reusing huge information tests can enable 
scientists to produce bits of knowledge. Conventional examples 
can't accomplish the measurable power that numerous labs need. 
This is a major test for brain research to enhance its strategies 
and replication process.

With Big Data, it gives you the chance to utilize a few 
extraordinary 
kinds of semi trial outlines, to help preclude elective 
clarifications, D'Onofrio said~\cite{editor00}.

D'Onofrio and his associates as of late adjusted the a great many 
individual records incorporated by Sweden with a specific end goal 
to challenge the customary 
thought that smoking amid pregnancy straightforwardly prompts 
resulting negative 
behavioral results, for example, wrongdoing~\cite{editor00}. 
In one examination, the scientists broke down 
50,000 kin whose moms smoked however did not smoke amid pregnancy. 
They decided the family setting factor, not smoking amid pregnancy,
identified with criminal feelings. This acknowledgment can 
significantly progress intercessions: 
For this situation, enabling ladies to stop smoking should just be
 a piece of more extensive social administrations.

Tal Yarkoni of the University of Texas at Austin says big data can 
even help psychologists study research. Yarkoni and others recently 
developed Neurosynth, an online program for analyzing a large number 
of fMRI data to guide users to topics of interest. Yarkoni said that 
so far, Neurosynth has synthesized studies from more than 9,000 
neuroimaging studies and about 300,000 brain activations~\cite{editor00}.

One of Neurosynth's main goals is to distinguish brain activity that 
is always associated with a specific psychological process but not 
specific, and high-probability brain activity that means having a 
specific mental process. For example, painful physical stimulation 
may continue to produce patterns of brain activity, but this 
pattern 
of activity does not necessarily mean the presence of pain; other 
mental states may produce similar patterns. Infer mental processes 
from observed brain activity, this process is called reverse 
reasoning, it is difficult to do in a single neuroimaging 
study~\cite{editor00}.

Neurosynth upset thinking by collecting countless 
pictures and research information in one place. For instance, 
regardless of whether a few 
dynamic cerebrum districts cover in these three cases, the database
 can enable specialists to discover zones of the mind that are 
especially important 
to torment, not working memory or feelings. Tests have demonstrate
 that, in numerous 
cases, Neurosynth's execution and investigation are done physically
 by screening research writing - sparing several hours of research 
time contrasted with simply squeezing a catch, Yarkoni said~\cite{editor00}.

The long term goal is the theme of the plan, because big data is 
still the existence of science. Not everyone believes that it will 
bring a paradigm shift. Even if big data does change statistical analysis, 
it cannot replace Strong behavioral theory or experiment. But in terms of 
improving these theories or strengthening big data for these experiments, 
researchers cannot ignore it.

\section{Simple Introduction of Psychology}

Psychology research is the logical investigation of the psyche and
 conduct. Psychology is a multifaceted train and incorporates 
numerous sub-fields of concentrate such zones as human advancement,
 sports, wellbeing, clinical, social conduct and intellectual 
procedures~\cite{editor01}. Psychology is extremely another 
science, with most advances occurring in the course of recent years
 or somewhere in the vicinity. Be that as it may, its starting 
points can be followed back to old Greece, 400 – 500 years BC. 
The accentuation was a philosophical one, with awesome masterminds,
 for example, Socrates impacting Plato, who thus affected 
Aristotle. Rationalists used to talk about numerous subjects now 
considered by present day brain science, for example, memory, 
through and through freedom, fascination and so forth. In the 
beginning of brain science there were two predominant hypothetical
 viewpoints. 

Rather, spotlight ought to be on how and why a creature 
accomplishes something. It was recommended that therapists should 
search for the basic reason for conduct and the psychological the 
procedures included. This accentuation on the causes and results of
 conduct has affected contemporary psychology.

In this area, professional practitioners or researchers are called 
psychologists and can be classified as social, behavioral, or 
cognitive scientists. Psychologists try to understand the role of 
mental function in personal and social behavior, and also explore 
the physiological and physiological processes behind cognitive 
function and behavior.

Psychologists explore behavioral and psychological processes, 
including perception, cognition, attention, emotion (emotion), 
intelligence, phenomenology, con lation, brain function and 
personality. This extends to human interactions such as 
interpersonal relationships, including psychological resilience, 
family resilience, and other areas. Psychologists in different 
directions also consider
 unconscious thoughts~\cite{editor02}. Psychologists use empirical 
methods to infer causality and correlations between psychosocial variables. 
In addition, or on the contrary, using experience and deduction methods,
 some clinical and counseling psychologists sometimes rely on 
symbolic explanations and other inductive techniques. Psychology is 
described as a central science whose psychological research results 
are linked to research and social sciences, natural sciences, 
medicine, humanities, and philosophy.

Although psychology knowledge is often used to assess and treat 
mental health problems, it also points to understanding and solving 
problems in several areas of human activity. Many people's psychology
 is ultimately for the benefit of society. Most psychologists are 
involved in clinical, counseling or some kind of treatment and 
practice in the school environment. Many people usually work in 
university psychology or other academic environments (such as medical
 schools, hospitals) and engage in extensive scientific research on 
psychological processes and behaviors. Some people are employed in 
industrial and organizational environments or other fields such as 
human development and aging, sports, health and media, as well as 
legal investigations and other aspects of the law.


\section{Certain Data Exploration methods useful in Psychological Research}

Data mining methods can be roughly divided into two categories: 
supervised learning methods and unsupervised learning methods. In 
supervised learning, there is an interesting result. The goal is to
 develop a predictive model based on a set of variables. Most 
supervised learning methods focus on variable selection, 
nonlinearity, and interactive effects, and therefore have many 
advantages over standard regression models. Regression models with
 a large number of variables may be unstable, especially if there 
is a high degree of correlation between predictors. In addition, 
when the number of variables is large, it may be almost impossible
 to manually search for which interactions may exist. The goal of 
supervised learning methods is to identify variables that are 
important variables, nonlinear forms and/or their interaction 
effects. These methods usually produce a simpler and more 
interpretable model because important influences can be isolated. 
In addition, the final model is more likely to be duplicated in the
 new sample.

In unsupervised learning, we do not want to explain any outcome 
variables; instead, our goal is to group variables or participants
 based on their similarities or covariances. Unlike supervised 
learning methods, unsupervised learning is often used in 
psychological research. For example, data reduction methods such as
 principal component analysis (PCA) and exploratory factor analysis
 (EFA) are very common in psychology~\cite{editor11}, 
and methods for grouping 
participants are also common, such as cluster analysis and finite 
mixture models.

Supervised learning methods are rarely used in psychology; however,
 these methods should and will play a greater role in future 
psychological research. As we pointed out, one reason that these 
methods may not be adopted in psychology is because researchers may
 think that these methods require large amounts of data - a large 
number of participants and a large number of variables. It is worth
 noting that many data mining methods work well in small data 
setups. For example, when considering losses due to losses, 
classification and regression trees exceeded multiple inferences in
 small samples. As a second example, it was found that the use of 
shrinkage in the modeling of Bayesian 
structural equations produces less biased samples than the maximum
 likelihood estimation.

As we have pointed out, unsupervised learning methods are very 
common in psychology. PCA and EFA are common data reduction 
methods, and EFA is often the first step in understanding the data
 dimension~\cite{editor11}. 
In many cases, the EFA model is applied to one-half of
 the data set, and then a confirmatory factor analysis (CFA) model
 is estimated based on the remaining half of the data to separate 
the exploration from the validation of the data analysis. This 
method is similar to cross-validation, but researchers in 
psychology often do not validate the exact model. In general, the 
model is re-estimated in the CFA, and the negligible factor load 
in the EFA is fixed at zero in the CFA.

Similar to PCA and EFA, cluster analysis and finite mixture models
 are common in psychology and social sciences. Limited-mix models 
are increasingly used to search for groups with different data 
patterns or associations. In psychology, few influences are 
universal~\cite{editor11}. 
The finite mixture model is a way for researchers to 
search for conditional effects. One problem with using the finite 
mixture model in psychology is that cross-validation is rarely used
 to assess the feasibility of the model. However, cross-validation
 has recently received more attention in hybrid modeling.

Although supervised learning methods are not commonly used in 
psychology, most of them can be attributed to the lack of attention
 paid by these methods from scholars of psychological science 
methods. As more and more data mining methods gradually apply to 
the nuances and complexity of psychological data and 
methods~\cite{editor11}, this situation is 
slowly and surely changing. Specifically, many of these big data 
approaches with latent variable models are focused common in 
psychology.

Due to our multivariate measurement and our fairly common vertical
 design, latent variable models (eg, confirmatory factor models, 
structural equation models [SEM]) are common in psychology. 
Combining data mining algorithms with latent variable models is a 
necessary step to increase the use of psychologists. There have 
been several recent examples of such integration. For example, 
Brandmaier, von Oertzen, McArdle and Lindenberger~\cite{editor11} 
developed SEM trees by combining SEM with classification and 
regression 
tree algorithms. In the SEM tree, the data is divided using a 
series of predictors, and the user-specified SEM fits into each 
partition of the data. The goal is to find the predictors that 
maximize the fit of the model. Essentially, this is a method of 
automatically searching for a group of participants, where members
 of the same group are homogeneous about SEM, and members of 
different groups are heterogeneous about SEM~\cite{editor11}. 
For example, a SEM tree can be used to
 find groups with different time trajectories or groups with 
different measurement models.

Similarly, Jacobucci, Grimm and McArdle (2016) combined 
regularization methods commonly used in high-dimensional regression
 with SEMs to create regularized SEMs (RegSEM)~\cite{editor11}. 
RegSEM allows 
researchers to punish specific parameters in the SEM, resulting in
 a simpler, more replicable SEM. There are similar developments in
 the multi-level modeling framework. For example, Hajjem, 
Bellavance and Larocque (2011) and Sela and Simonoff (2012) 
combined
 a mixed effects model with a regression tree to create a 
mixed-effect regression tree~\cite{editor11}. 
These methods can efficiently search
 high-dimensional hierarchically structured data for nonlinear and
 interactive effects.

Although recent work has made some algorithms more applicable to 
social scientists, we emphasize the difficulty of gaining 
attention - incomplete data. In summary, many data mining 
algorithms require complete data. Moreover, different programs 
handle incomplete data in different ways. Since incomplete data are
 common in psychological research and are usually not lost 
completely at random, the model may produce biased results, or at 
least the result depends on the data used to deal with 
incompleteness. Therefore, an approach to future research will 
greatly increase the usefulness of these methods in psychological 
research, incorporating contemporary missing data methods (such as
 multiple attribution or comprehensive information estimates) into
 data mining plans.

\section{Application of Data Mining to Predict Real Life Outcomes}

This section is based on the study of Mining Big Data to Extract 
Patterns and Predict Real-Life Outcomes~\cite{editor12}.

Human exercises are progressively influenced by advanced items and 
administrations~\cite{editor13}. 
People utilize interpersonal interaction destinations to impart 
with informing applications, utilize online stages and Mastercards 
for installment, and transmit computerized media. Likewise, they 
are for all intents and purposes indistinguishable from wearable 
gadgets, for example, wellness 
trackers and advanced mobile phones. Progressively drenched in the
 advanced 
condition and depending on computerized gadgets implies that 
individuals
conduct, correspondence, geographic area and even physical 
status can be effectively recorded, bringing about countless 
computerized impression tests. These impressions incorporate web 
perusing 
logs, exchange records from on the web and disconnected markets, 
photographs 
what's more, recordings, GPS area logs, media playlists, voice and
 video 
call records, dialects utilized as a part of tweets or messages, 
and that's only the tip of the iceberg.

Late research shows that advanced impressions can be effectively 
utilized to consider critical mental results 
extending from digital footprint~\cite{editor14}, 
language~\cite{editor15}, and feelings~\cite{editor16}  
to social fit~\cite{editor17} and long range informal communication. 
Lamentably, an intrigue in concentrate computerized impressions, 
and in addition the vital aptitudes 
to do as such, are generally uncommon among social researchers. Thusly, 
such research is progressively surrendered to PC researchers and 
engineers, who regularly do not have the hypothetical foundation in
 social scienceand preparing in moral gauges relating to human 
subjects explore.

The following of this section will describe the steps of applying 
big data technique in psychology, most of the codes are written by R.

\subsection{Data Sets of Digital Footprints}

This subsection focuses on how to import, store and preprocess 
large 
samples of digital footprints. Many large data sets can be obtained
 online for free, or can be obtained from companies and 
institutions that collect and store them. In a popular example, 
myPersonality.org database 1 stores scores from dozens of 
psychological questionnaires and Facebook profile data for more 
than six million participants. The Stanford University Web 
Analytics Project website2 has a wide range of datasets, including
 social networking, Tweets, and product reviews. In addition, 
journal articles are now often accompanied by publicly available 
data sets. A recent article by Eichstaedt et al. (2015), 
supplemented by data sets aggregated at the county level by U.S. 
users.3 In addition, many online platforms (such as Twitter) 
contain large amounts of publicly available data that can be easily
 recorded. Peer website provides a collection links to other 
potentially interesting data sets. The following hands-on section 
describes the sample data set used in this tutorial. The sample 
data set was obtained from the myPersonality.org database.

The data set used here contains psychodemographic profiles of 
110,728 Facebook users and their Facebook Likes.For simplicity and
 ease of management, the sample is limited to U.S. users. The 
following three files can be downloaded from the companion website:

1.\ user.csv: Contains psychodemographic user profiles. It has 
110,728 lines (not including line and column names) and 9 columns:
 anonymous user ID, gender (male for 0, female for 1), age, 
political opinion (Democrats for 0, and 1 for Republicans) 
as well as five scores in the 100-item International Personality 
Program Questionnaire to assess five factors (ie, openness, 
discretion, extroversion, agreeability, and neuroticism) 
personality models~\cite{editor18}.

2.\ likes.csv: Contains anonymous ID and 1,580,284 Facebook like 
names. It has two columns: ID and name.

3.\ users-likes.csv: Include associations between users and their 
likes, stored as user class pairs. It has 10,612,326 rows and two 
columns: User ID and Like ID. The existence of a similar user pair
 means that a given user has a corresponding Like on their profile.

Using R to load the data files:

\begin{verbatim}
users <- read.csv('users.csv')
likes <- read.csv('likes.csv')
ul <- read.csv('users-likes.csv')
\end{verbatim}

This code is quoted from~\cite{editor12}.

Constructing a User–Footprint Matrix

\begin{verbatim}
ul$user_row <- match(ul$userid,users$userid)
ul$like_row <- match(ul$likeid,likes$likeid)
\end{verbatim}

This code is quoted from~~\cite{editor12}.

Use the pointers to rows in the users and likes objects to build 
a user–Like matrix using the sparseMatrix function from the Matrix
 library:

\begin{verbatim}
require(Matrix)
M <- sparseMatrix(i=ul$user_row, j=ul$like_row, x=1)
\end{verbatim}

This code is quoted from~\cite{editor12}.

The parameters i and j of the sparseMatrix function indicate the 
positions of the non-zero cells of the matrix (rows and columns, 
respectively). The parameter x represents the value of each cell. 
As mentioned earlier, Facebook users can only issue Like once, so 
we set all non-zero units to x equal to 1.

Finally, the row-name of the similar user's Matrix M is set to 
contain the ID of each user, and the column name of M is set to 
contain each favorite name.

\begin{verbatim}
rownames(M) <- users$userid
colnames(M) <- likes$name
dim(M)
\end{verbatim}

This code is quoted from~\cite{editor12}.

\subsection{Trimming the User–Footprint Matrix}

There is no single or simple, correct way to select the lowest 
frequency, below which frequency a given user or step should be 
deleted, but the following rules can be used to make the decision.
 First, remove individual user and footstep instances from the 
data because they are not useful for the extraction mode. Second, 
consider the available hardware and computing time. Keeping too 
many data points may increase the time and memory required. On the
 other hand, deleting too many data points can significantly reduce
 the amount of information available for analysis. Therefore, it is
 recommended to plan and analyze a small number of randomly 
selected small samples of different sizes in order to approximate 
the relationship between the required data size and time and 
memory. This not only tells you how much data you need to keep, 
but also speeds up writing code by reducing the time required for 
testing.

\subsection{Hands-On: Trimming the User–Like Matrix}

\begin{verbatim}
repeat {
i <- sum(dim(M))
M <- M[rowSums(M) >= 50, colSums(M) >= 150]
if (sum(dim(M)) == i) break
}
\end{verbatim}

This code is quoted from~\cite{editor12}.

This code uses repeated loops until interrupted by a command 
interrupt. Inside the loop, we first set i to contain the sum of 
the dimensions of M (that is, the total number of rows and 
columns). Next, we only keep the rows and columns that contain at 
least the elements of the preset thresholds 50 and 150. Finally, 
we check if the size of M has changed. If so, the loop will break;
 otherwise, it will continue.
Next, users deleted from M are removed from the users object:

\begin{verbatim}
users <- users[match(rownames(M),
users$userid),]
\end{verbatim}

This code is quoted from~\cite{editor12}.

\subsection{Extracting Patterns from Big Data Sets}

This subsection focuses on two approaches that represent two large 
families, extracting patterns from the user's footprint matrix: 
(a) Singular value decomposition, which represents a method based 
on feature decomposition, projecting a set of data points into a 
set of dimensions; and (b) The potential Dirichlet distribution 
represents a cluster analysis method. The main advantages of LDA 
and other cluster analysis methods are easy to explain. However, 
they are generally computationally expensive and can only handle 
non-negative data. (Fortunately, in the context of digital 
footprints, there is very little negative data.) The main 
advantages of the singular value decomposition based on internal 
decomposition and many other methods are its simplicity and speed 
of calculation. Therefore, they are often used to develop 
predictive models. Compared with LDA, the method based on 
intrinsic decomposition can also be applied to data sets containing
 negative data points.

Reducing the dimensionality of the data (or extracting the cluster)
 has many advantages. First, in the case of large data sets, there
 are usually more variables than users. In this case, reducing the
 dimension is very important because most statistical analysis 
requires more users than variables (preferably more). Second, even
 if the number of users exceeds the number of variables, further 
reductions can reduce the risk of overfitting and may increase the
 statistical power of the results. Third, reducing dimensions by 
eliminating multiple collinearity and redundancy in the data group
 related variables into a dimension or cluster. Fourth, a small 
number of dimensions or clusters containing data are easier to 
interpret than hundreds or thousands of individual variables. 
Finally, reducing the dimensions reduces the computational time and
 memory required for further analysis.

\subsection{Selecting the Number of Dimensions or Clusters to Extract}

One of the main considerations for reducing the data dimension is 
to select the correct number of dimensions or clusters (in k) to be
 extracted. Unfortunately, there is no single (or simple) correct 
way. Moreover, the expected value of k depends on the intended 
application. If the goal is to gain insight from the data, several
 dimensions or clusters may be easier to interpret and visualize. 
On the other hand, if the goal is to build a predictive model, more
 dimensions or clusters will retain more information from the 
original matrix to achieve more accurate predictions. However, k is
 set too high, and the benefits of dimension reduction discussed 
earlier are lost, and the prediction accuracy may be reduced. The 
following sections discuss SVD and LDA in more detail and introduce
 some simple ways to choose the correct value for k.

\subsection{Singular Value Decomposition}

SVD is a popular dimensionality reduction technique that is widely
 used in various environments, including computational social 
sciences, machine learning, signal processing, natural language 
processing, and computer vision. Social scientists are usually more
 familiar with similar methods: Principal Component Analysis (PCA).
 In fact, PCA can be considered as a special case of SVD: SVD is 
performed on the centered matrix to generate V and $\sum$, 
which is equivalent
 to the feature vector and square root of the feature values 
generated by PCA. Unfortunately, PCA needs to multiply the matrix 
by its transpose, which is computationally inefficient for large 
matrices.

SVD denotes a given matrix of the products of three matrices 
(m rows
 and n columns): The matrix U (with size mk) containing the left 
singular vector contains a non-negative square diagonal matrix 
(size k); the matrix V (size nk) contains The correct singular 
vector, where k is the dimension that the researcher chooses to 
extract. For simplicity, we refer to the left and right singular 
values as SVD dimensions.

\subsection{Centering the data}

Before performing SVD to improve the interpretability of the SVD 
dimension, the data is usually centered (ie, the entries in the 
matrix are reduced column by column). This is because the first SVD
 dimension extracted from non-central data is closely related to 
the frequency of objects in rows and columns.11 Since the remaining
 dimensions must be orthogonal to the first dimension dimension, 
the resulting SVD dimension may not be well represented data.

\subsection{Rotation}

The SVD aims to maximize the variance considered by the first and 
subsequent orthogonal dimensions. Therefore, early SVD dimensions 
are highly relevant to many users and footprints. In addition, many
 users and footprints are related to many SVD dimensions, making 
SVD results difficult to interpret.

\subsection{Computing SVD}

Most programming languages provide ready-made packages for 
calculating SVD: the svd function of Python's SciPy library, the 
feature library in C, the PROPACK library in Matlab, and the 
irlba package in R. For sparse or very large matrices, it is 
recommended to use SVD sparse variants available in most of the 
above packages. You can use the various functions available in R 
(for example, varimax or promax), Matlab, and other languages for 
factor rotation.

\subsection{Latent Dirichlet Allocation}

LDA assumes that each user and each footstep in the matrix (in the
 case of LDA, the user is referred to as a document, and the 
footprint as a word) belongs to a group of k clusters (known as 
topics) with a certain probability. For matrices of size m rows and
 n columns, the LDA generates a matrix of size mk, describing the 
probability that each user belongs to each cluster, and a matrix of
 size kn that describes the probability that each cluster belongs 
to each cluster. 

\subsection{Selecting the k}

As with other methods, there is no single or simple way to select 
the correct number k of LDA clusters to extract. Common methods use
 the log-likelihood estimate of the model and can plot the number k
 of clusters extracted. This requires the generation of several 
models for different k values. Generally, for a lower range of k, 
the log-likelihood grows rapidly, flattens at higher k-values, and
 may begin to decrease once the number of clusters becomes very 
large. Choosing a marker that quickly increases the log-likelihood
 at the end of k usually explains these topics well. Larger 
k-values generally provide better predictive power. Alternatively,
 the interpretability of the topic may be supported by applying a 
hierarchical LDA.

\subsection{Computing LDA}

Most programming languages provide ready-made LDA implementations: Rs 
theme model package, Pythons scikit-learn package, 
GibbsLDA++ for C++, and theme modeling toolbox for Matlab.

\subsection{Hands-On: Reducing the Dimensionality of the
User–Like Matrix Using SVD and LDA}

\begin{verbatim}
set.seed(seed = 68)
library(irlba)
Msvd <- irlba(M, nv = 5)
u <- Msvd$u
v <- Msvd$v
\end{verbatim}

This code is quoted from~\cite{editor12}.

The following code produces Vrot and Urot, the varimaxrotated
equivalents of matrices U and V

\begin{verbatim}
v_rot <- unclass(varimax(Msvd$v)$loadings)
u_rot <- as.matrix(M %*% v_rot)
\end{verbatim}

This code is quoted from~\cite{editor12}.

As in the case of the SVD analysis, seed 68 was used as the preset
 R’s random number generator to ascertain that the results 
presented here and computed by the readers are the same:

\begin{verbatim}
library(topicmodels)
Mlda <- LDA(M, k = 5, control = list(alpha =
10, delta = .1, seed = 68), method =
'Gibbs')
gamma <- Mlda@gamma
beta <- exp(Mlda@beta)
\end{verbatim}

This code is quoted from~\cite{editor12}.

Consider expanding the test scope of ks, but note that this code 
may take a long time to run:

\begin{verbatim}
lg <- list()
for (i in 2:5) {
Mlda <- LDA(M, k = i, control =
list(alpha = 10, delta = .1, seed = 68),
method = 'Gibbs')
lg[[i]] <- logLik(Mlda)
}
plot(2:5, unlist(lg))
\end{verbatim}

This code is quoted from~\cite{editor12}.

\subsection{Interpreting Dimensions and Clusters}

This section focuses on the dimensions and clustering extracted 
from the user's footprint matrix. This explanation is certainly not
 trivial, but it may provide important insights. Several major 
psychological models and theories are derived from different 
analyses than those provided in this paper. For example, the 
lexical assumption assumes that important interpersonal 
psychological differences become part of the language. This theory
 is an important foundation for many personality psychology and was
 used to develop major personality frameworks such as HEXACO or Big
 Five. Expected interpersonal psychological differences and other 
psychological phenomena can also reflect the reasonable digital 
footprint in the model.

Several strategies can be used to explain the nature of patterns 
extracted from large data sets. First, you can explore the 
footprints that are most relevant to a given dimension or cluster.
 This method is used to explain the dimension and clustering 
extracted from the user movie Matrix X in the extracting schema 
part from the big data set. Second, you can examine the 
relationship between dimensions and clusters as well as some of 
the user's known properties (for example, demographic information,
 psychological characteristics, responses to individual questions 
on the psychological questionnaire, etc.). Finally, you can 
explore the interpretation of dimensions or clusters by exploring 
them a hierarchy that spans different k levels. The equivalent of 
LDA is provided by graded LDA.

\subsection{Hands-On: Interpreting Clusters and Dimensions}

\begin{verbatim}
cor(gamma, users[,-1], use = 'pairwise')
\end{verbatim}

This code is quoted from~\cite{editor12}.

LDA1 members were negatively correlated with age, positively 
correlated with gender (male 0, female 1), political opinion 
(democrats 0, Republicans 1), indicating that this group was mainly
 reserved for young people. Female. Cluster LDA2 is similar but has
 nothing to do with gender. The group LDA3 contains elderly, 
liberal and politically free men. Compared with other groups, the 
correlation of LDA4 with the psychological population 
characteristics included in this sample was low. The last cluster 
LDA5 has the closest relationship with the personality dimension. 
Emotionally stable, happy, outgoing, responsible, conservative 
(including personality and political views), older and female users.

The quality of the connection amongst preferences and bunches is 
put away in the R protest beta. The accompanying code can be 
utilized to extricate the main 10 Likes that are most intently 
connected with each cluster:

\begin{verbatim}
top <- list()
for (i in 1:5) {
f <- order(beta[i,])
temp <- tail(f, n = 10)
top[[i]] <- colnames(M)[temp]
}
top
\end{verbatim}

This code is quoted from~\cite{editor12}.

\subsection{SVD dimensions}

A similar approach can be used in the SVD dimension. Use the 
following code to obtain the correlation between the maximum 
likelihood rotated SVD dimension (Urot) and the user score for the
 mental demographic user function:

\begin{verbatim}
cor(u_rot, users[,-1], use = 'pairwise')
\end{verbatim}

This code is quoted from~\cite{editor12}.

Just as in the case of LDA clusters, one can also study the 
association between the like and the maximum-maximum rotation SVD 
dimension. For the benefit of space, we do not include these 
results here, but encourage readers to make them. Please note that
 Vrot is an equivalent in LDA. Moreover, since SVD produces a 
negative correlation and a positive correlation, it is convenient 
to separately consider the most positive and negatively related 
footprints for a given SVD dimension.

The loop previously used to extract the loop of the most 
representative LDA cluster can be used to extract the likes of the
 10 highest scores and 10 lowest scores in the SVD dimension. To 
rank likes based on their score on the ith SVD dimension, use the 
following command:

\begin{verbatim}
f <- order(v_rot[,i])
\end{verbatim}

This code is quoted from~\cite{editor12}.

To extract a favorite index with extreme scores, use the tail and 
head functions to provide the n last or first elements of the 
object:

\begin{verbatim}
colnames(M)[tail(f, n = 10)]
colnames(M)[head(f, n = 10)]
\end{verbatim}

This code is quoted from~\cite{editor12}.


\subsection{Predicting Real-Life Outcomes}

This part focuses on building a prediction model based on 
dimensions and clusters extracted from the user's footprint matrix.
 14 A large number of methods can be used to build predictive 
models based on large datasets, from relatively complex methods 
(such as deep learning, neural networks, probability map models, or
 support vector machines) to simpler methods (such as linear and 
logistic regression). In practice, it is wise to start with a 
simple forecasting method. They are faster and easier to implement,
 and they provide a good benchmark for judging quality and 
debugging more complex methods. In addition, according to our 
experience, simple models (such as linear and logistic regression)
 often provide similar accuracy for more complex methods - and are
 easier to interpret. Finally, using simple methods can reduce the
 risk of errors, maximize the transparency of the method, and 
promote the reproducibility of results. In this work, we use linear
 and logistic regression, which is well known to most social 
scientists.

\subsection{Cross-Validation}

To reduce the variability of results, multiple rounds of 
cross-validation are often performed using different data 
partitions. For example, k-fold cross-validation divides the data 
into k (usually k = 10) subsets of the same size (called folds). 
The model was built on a training subset consisting of all 
one-to-one (k-1) folds and verified on the excluded subset of 
tests. This process is repeated k times for each subset and the 
accuracy is averaged over all trials.

Cross-validation can be easily done in R and other statistical 
languages, such as using for loops, but there are also 
well-specified libraries. Popular R libraries that support 
cross-validation include boot and caret. For Python users, we 
recommend using the cross-validation module in scikit-learn.

\subsection{Hands-On: Predicting Real-Life Outcomes With
Facebook Likes}

The following code generates a vector collapse with a length equal
 to the number of users, consisting of randomly selected numbers 
ranging from 1 to 10:

\begin{verbatim}
folds <- sample(1:10, size = nrow(users),
replace = T)
\end{verbatim}

This code is quoted from~\cite{editor12}.

Next, users with a folding value equal to 1 are assigned to the 
test subset and the remaining users are assigned to the training 
subset. The following code generates the logic vector15 test, if 
the object is collapsed to 1, the test result is TRUE, otherwise 
returns FALSE:

\begin{verbatim}
test <- folds == 1
\end{verbatim}

This code is quoted from~\cite{editor12}.

Logical vectors can be used to extract the desired elements from R
 objects, such as other vectors or matrices. For example, you can 
use the command mean (users age [test]) to calculate the average age
 of users in the test subset. In addition, the true/false elements
 of the logic vector can be easily represented using logical 
operators rather than [!], so instead of creating a separate 
logical vector vector indicating the degree of membership in the 
training subset, you can simply use the test! test. In the 
following code, test and test vectors are used to access test and 
training subsets, respectively.

Therefore, the SVD scores of the users in the test subset are based
 on the analysis performed only on the training subset, which 
preserves the independence of the results obtained from the 
training and test subsets:

\begin{verbatim}
Msvd <- irlba(M[!test,], nv = 50)
v_rot <- unclass(varimax(Msvd$v)$loadings)
u_rot <- as.data.frame(as.matrix(M %*%
v_rot))
\end{verbatim}

This code is quoted from~\cite{editor12}.

Finally, it is pointed out that all the independent variables 
should be used to predict the variable ope in the object user.

\begin{verbatim}
fit_o <- glm(users$ope, data = u_rot,
subset = !test)
fit_g <- glm(users$gender, data = u_rot,
subset = !test, family = 'binomial')
\end{verbatim}

This code is quoted from~\cite{editor12}

Generate predictions using the models (fito and fitg) developed in
 the previous step and the user's maximum rotation SVD score 
(urot [test,]) in the test subset:

\begin{verbatim}
pred_o <- predict(fit_o, u_rot[test,])
pred_g <- predict(fit_g, u_rot[test,], type =
'response')
\end{verbatim}

This code is quoted from~\cite{editor12}

The accuracy of linear prediction (eg, openness) can be easily 
expressed as a Pearson product moment correlation:

\begin{verbatim}
cor(users$ope[test], pred_o)
\end{verbatim}

This code is quoted from~\cite{editor12}

So far, we estimate the prediction performance based on all the SVD
 dimensions of k = 50. Here, we will study the relationship between
 cross-validation prediction accuracy and the SVD dimension k. In 
the following code snippet, we first define a set ks containing the
 k values we want to test. It includes all values from 2 to 10, 15,
 20, 30, 40 and 50. Next, we create an empty list rs to store the 
results. Then we start a for loop, rerun the code surrounded by 
braces, and change the value of i to a continuation element stored
 in ks. The code in parentheses is similar to the code used 
previously, but only the first of the k = 50 SVD dimensions (Msvd 
 v [,1:i]) is used. The result is stored as a new element in the 
list rs (rs [[as.character (k)]]). We fix R's random number generator
 again to ensure that our results match the reader's results:

\begin{verbatim}
set.seed(seed = 68)
ks<-c(2:10,15,20,30,40,50)
rs <- list()
for (i in ks){
v_rot <- unclass(varimax(Msvd$v[,
1:i])$loadings)
u_rot <- as.data.frame(as.matrix(M%*%
v_rot))
fit_o <- glm(users$ope, data = u_rot,
subset = !test)
pred_o <- predict(fit_o, u_rot[test,])
rs[[as.character(i)]] <- cor(users$ope
[test], pred_o)
}
\end{verbatim}

This code is quoted from~\cite{editor12}

Finally, the open cross-validation was calculated forecast for the 
entire sample. To do this,  a model was build and used the data 10 
times to calculate the predicted value. First, a vector was created
predo to store the prediction of the entire sample. It is full of 
missing values (NA) and has as many elements as the number of 
users. Next, we use the for loop to re-run the previously described
 code while selecting a continuous cross-validation fold. Each 
time, the predicted openness value is saved as an element of the 
predo vector. After the loop is completed, the correlation was 
caculated between the actual open score and the predicted open 
score for all users in the sample:

\begin{verbatim}
set.seed(seed = 68)
pred_o <- rep(NA, n = nrow(users))
for (i in 1:10){
test <- folds == i
Msvd <- irlba(M[!test,], nv = 50)
v_rot <- unclass(varimax(Msvd$v)$loadings)
u_rot <- as.data.frame(as.matrix(M %*%
v_rot))
fit_o <- glm(users$ope, data = u_rot,
subset = !test)
pred_o[test] <- predict(fit_o, u_rot[test,])
}
cor(users$ope, pred_o)
\end{verbatim}

This code is quoted from~\cite{editor12}

The prediction accuracy for openness, estimated on the entire
sample, is r = 0.44.

Try wrapping it within a for loop, similar to the one used for 
producing SVD-based predictions:

\begin{verbatim}
Mlda <- LDA(M[!test,], control =
list(alpha = 1, delta = .1, seed = 68),
k = 50, method = 'Gibbs')
temp<-posterior(Mlda, M)
gamma <- as.data.frame(temp$topics)
\end{verbatim}

This code is quoted from~\cite{editor12}

\section{Conclusion and Discussion}

Therefore, Big data is very likely to provide value to psychology.
 However, the pursuit of big data is still an uncertain and risky 
career for ordinary psychology researchers.

How can psychology adapt to the field of big data or computing 
social sciences and other related fields? Psychology has begun to 
cover many areas such as health, mental health, depression, 
substance use, behavioral health, behavioral change, social media,
 workplace well-being and effectiveness, student learning and 
adjustment, and behavioral genetics. Researchers are working on 
topics such as health and human conditions in large data sets that
 contain thousands of people. Researchers envision research that 
can link these personal data with health and productivity, 
revealing patterns or connections between behavior and various 
outcomes of interest.

Obviously, big data or data science stay here, with or without 
psychology. This broad and growing field provides interested 
thinkers with a unique opportunity to address the complex 
technical, substantive, and ethical challenges associated with 
storing, retrieving, analyzing, and validating large data sets. 
Big data science can jointly discover and elucidate the persuasive
 and powerful patterns in psychological data that directly or 
indirectly involve human behavior, cognition, and the effects of 
time and socio-cultural systems. In turn, these mental models give
 non-psychological data (such as medical data related to 
health-related interventions; prosperity and depression related to
 financial investment behavior). The big data community and big 
data itself can jointly promote the development of psychological 
science.


\begin{acks}

  The authors would like to thank Dr.~Gregor~von~Laszewski for his
  support and suggestions to write this paper.

\end{acks}


\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

