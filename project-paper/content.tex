% status: 0
% chapter: TBD

\title{Paper: Psychology and applying certain Big Data 
algorithms in Psychology data}


\author{Qingyun Lin}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{Smith Research Center}
  \city{Bloomington} 
  \state{IN} 
  \postcode{47408}
}
\email{ql10@iu.edu}


% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{G. v. Laszewski}


\begin{abstract}
There is much discussion about the value of company big data. 
For example, Amazon will match your shopping and page views with 
other shoppers’ purchases and pageviews, and try to find people with 
similar interests. Then Amazon recommends buying products that people 
like, assuming you will like them.

Can big data be applied to answer the research community’s interest 
in psychology?

This paper will list some experts opinions of applying big data 
technique in psychology, simply introduce what is psychology, 
introduce certain data exploration algorithms that are useful in 
psychology datasetsand an R example of applying big data algorithms 
will be displayed, and the problems of applying big data technique 
in psychology will be listed as examples at the end of the paper. 

\end{abstract}

\keywords{hid-sp18-515, big data, pychology, combination, future}


\maketitle

\section{Introduction}

Pychology problem is getting more and more attention in the morden
society. And as the big data technology is such a heated topic 
today, the combination of this two fields should be well applied.
Big data has the advantage to do the survays that with the question 
that people may be not willing to answer. 
Often, the way people use computers shows the charactors that they
 would not express in face to face surveys.

Computer science has been improved, which is coupled with the 
proliferation of large internet and social media platforms,
 have produced an unimaginable amount of people's infomation.
It is estimated that the amount of digital data currently present 
is in the thousands of exabytes or 10 to the 18th power of bytes
~\cite{hid515-00}.

\subsection{Examples of field in Psychology to apply big data technique}

In this time, big data is likely to change how psychologists 
collect the data of people's behavior. 
However, like it generates new 
chances, getting access to a large amount of information also creates
 new difficulty to research. 

Michael N. Jones of 
Indiana University Bloomington introduced the Big Data Project at 
the 2014 APS Annual Conference.
Each and every piece of data is a trace of human direct and 
offers us a potential snippet of data to comprehension 
fundamental mental gauges, said Jones~\cite{hid515-00}. 
However, it is necessary to gather every last one of 
those pieces adequately to better understand the major mental 
tenets that made them.

The language development, which is done by Jones's, is a good big data
 research case. 
Collecting a large number of infant samples from the natural 
environment is time-consuming and often results in small sample 
sizes. The test theory of how children study a new languages is a 
time consuming test.

Big data technique can speed up the process. 
Some concept, which is done by Jones, proves that based on associative
 learning theory, more than 
100,000 words from natural language can be entered into a computer
 model - the idea is that children group words together according 
to how often they use them near other words. 
Jones said that as the
 analysis progressed, the model did realize that the relationship 
between computers and data and word classes was closer than we can 
compute and obsolete~\cite{hid515-00}.

 A similar analysis can be used to study
 the theory of connected learning in direct child conversation 
samples. As long as they have enough data to continue, the 
models do better jobs about learning based on noised.

Tanzeem Choudhury, an information scientist at Cornell University, 
said that big data may help researchers reach the point where they 
can collect behavioral information without sampling human 
participants~\cite{hid515-00}. Technologies such as smart phones are 
able to collect information about real world behaviors and other 
infomation.

It is able to persistently get conduct estimations with not 
bothering individuals to round out reviews. 
It is possible to get persistent estimation without really 
engaging clients with a lot of time~\cite{hid515-00}.

Chowdhury participated in some of these projects. StressSense can 
track 
daytime stress and help them avoid anxiety. For example, MyBehavior uses 
physical behavior patterns to suggest methods of maintaining the shape, for 
example, working more frequently along lines that the user seems to like. 
MoodRhythm allows patients with bipolar disorder to monitor 
sleep and social 
interactions to maintain emotional and energy balance, which is a major 
improvement in pen and daily behavioral tracking~\cite{hid515-00}. 

Big data has produced positive changes in the online search field. 
The bunch of searching behavior on Internet 
that happen each day create the behavior log that helps
analysts improve searching algorithms. 

With data mining the behavior logs, experts are able to create 
individual algorithms 
to upgrade the user's experience of searching. If a person searches
certain words and switch a lot, the system will know that words
 do not always predict the best way users want to search. 
Modeling the search using the way 
which considers contexts of the published question are very 
significant for 
upgrading Websites searching. 
The search activity, which is done before, is important, and the 
time and location of the query are also important. As an example, 
the search for Spring US Open may refer to a golf ball, 
howerver, the similar 
search which is done in summer may want to get tennis.

Before the Big Data could be gathered, the individual who talked 
loudest, or then again the most generously compensated individual's
 assessment, would overwhelm, said Dumais. Presently the 
information, particularly when gotten from precisely controlled
 Web-scale tests, overwhelms. Enormous Data likewise
 enables scientists to consider the old issues in new perspactive, said 
Brian M. D'Onofrio, an APS specialist at Indiana University 
Bloomington. Specifically, he brought up that specialists ought to
 consider re-modifying information that might be gathered for 
different reasons. Reusing huge information tests can enable 
scientists to produce bits of knowledge. Conventional examples 
can't accomplish the measurable power that numerous labs need. 
This is a major test for brain research to enhance its strategies 
and replication process~\cite{hid515-00}.

With Big Data, it gives you the chance to utilize a few 
extraordinary 
kinds of semi trial outlines, to help preclude elective 
clarifications, D'Onofrio said~\cite{hid515-00}.

D'Onofrio and his associates as of late adjusted the a great many 
individual records incorporated by Sweden with a specific end goal 
to challenge the customary 
thought that smoking amid pregnancy straightforwardly prompts 
resulting negative 
behavioral results, for example, wrongdoing~\cite{hid515-00}. 
In one examination, the scientists broke down 
50,000 kin whose moms smoked however did not smoke amid pregnancy. 
They decided the family setting factor, not smoking amid pregnancy,
identified with criminal feelings. This acknowledgment can 
significantly progress intercessions: 
For this situation, enabling ladies to stop smoking should just be
 a piece of more extensive social administrations.

Tal Yarkoni of the University of Texas at Austin says big data can 
even help psychologists study research. Yarkoni and others recently 
developed Neurosynth, an online program for analyzing a large number 
of fMRI data to guide users to topics of interest. Yarkoni said that 
so far, Neurosynth has synthesized studies from more than 9,000 
neuroimaging studies and about 300,000 brain activations~\cite{hid515-00}.

One of Neurosynth's main goals is to distinguish brain activity that 
is always associated with a specific psychological process but not 
specific, and high-probability brain activity that means having a 
specific mental process. For example, painful physical stimulation 
may continue to produce patterns of brain activity, but this 
pattern 
of activity does not necessarily mean the presence of pain; other 
mental states may produce similar patterns. Infer mental processes 
from observed brain activity, this process is called reverse 
reasoning, it is difficult to do in a single neuroimaging 
study~\cite{hid515-00}.

Neurosynth upset thinking by collecting countless 
pictures and research information in one place. For instance, 
regardless of whether a few 
dynamic cerebrum districts cover in these three cases, the database
 can enable specialists to discover zones of the mind that are 
especially important 
to torment, not working memory or feelings. Tests have demonstrate
 that, in numerous 
cases, Neurosynth's execution and investigation are done physically
 by screening research writing - sparing several hours of research 
time contrasted with simply squeezing a catch, Yarkoni 
said~\cite{hid515-00}.

The long term goal is the target of the plan, because big data 
remains as the existence of science. Not anyone thinks that it can 
bring a significant change. Even though big data helps the 
statistic a lot, 
it cannot take the place of behavioral theory and experiment. 
However, in terms of improving these theories or strengthening 
big data for these experiments, researchers cannot ignore it.

\subsection{Simple Psychology Introduction}

Psychology research focuses on logical psyche investigation. 
Psychology is a multifaceted train and incorporates 
numerous sub-fields of concentrating 
such zones that human sports, advancement, clinical, wellbeing, 
social conduct and intellectual 
procedures~\cite{hid515-01}. Psychology, which is extremely another 
science, has advances occurring recently. Its starting
points may be tracked back to old Greece, 400 – 500 years 
BC~\cite{hid515-01}. 

Rather, spotlight ought to be on how and why a creature 
accomplishes something. It was recommended that therapists should 
search for the basic reason for conduct and the psychological the 
procedures included. This accentuation on the causes and results of
 conduct has affected contemporary psychology.

In this area, professional practitioners or researchers are called 
psychologists and may be classified as behavioral, social, or 
cognitive scientists. Psychologists try to know how the  
mental function works on personal and behavior, and the explore 
the physiological processes behind behavior.

Psychologists use empirical methods to infer causality and 
correlations between psychosocial variables. 
In addition, or on the contrary, using experience and deduction 
methods, some clinical and counseling psychologists sometimes rely
 on symbolic explanations and other inductive techniques. 
Psychology is described as a central science whose psychological 
research results are linked to research and social sciences, 
natural sciences, medicine, humanities, and philosophy.

Although psychology knowledge is often used to assess and treat 
mental health problems, it also points to understanding and solving
problems in several areas of human activity. Many people's 
psychology
 is ultimately for the benefit of society. Most psychologists are 
involved in clinical, counseling or some kind of treatment and 
practice in the school environment. Many people usually work in 
university psychology or other academic environments (such as medical
 schools, hospitals) and engage in extensive scientific research on
 psychological processes and behaviors. Some people are employed in
 industrial and organizational environments or other fields such as
 human development and aging, sports, health and media, and 
legal investigations and other aspects of the law~\cite{hid515-19}.

\section{Requirement of this Paper}
There are many ways and methods in psychology have an intensified 
potential to combine with big data technology. In this paper, it is to
 list certain examples and will list certain big data algorithms to 
explore and generate a model to predict in psychology dataset. 

\section{Certain Data Exploration methods 
useful in Psychological Research}

Data mining can be divided to two categories: 
unsupervised learning methods and supervised learning methods. The
 goal of supervising learning is developing a predictive mode, 
which is generated by a set of variables. The majority of 
supervised learning algorithms focus on selecting the variable, 
and they are nonlinearity, with interactive effects, 
and therefore they are better than traditional regression models in
many perspective. Regression models, which has many variables may 
cause unstablility, especially if the correlation between predictors 
is high. In addition, 
when the variables have high diversity, it may be almost impossible
 to manually find which interactions may exist~\cite{hid515-11}. 
The target of 
supervised learning is to find variables that are 
significant variables. 
These methods usually produce a easier and more 
understandable model. 
In addition, the final model is more likely to be duplicated in the
 new sample~\cite{hid515-11}.

We do not want to explain any outcome variables for unsupervising 
learning; In the contrast, 
our goal is to cluster the variables or participants
 based on their similarities or covariances. 
Different from the 
other one, unsupervised learning focuses more in 
psychological research. For example, data reduction methods such as
 principal component analysis (PCA) and exploratory factor analysis
 (EFA) are very common in psychology, 
and methods for grouping 
participants are also common, such as cluster analysis and finite 
mixture models~\cite{hid515-11}.

Supervised learning methods are seldomly used in psychology; 
nevertheless,
 these methods ought to play a greater role in future. As it is  
pointed out, why supervising learning may not be adopted in 
psychology is due to researchers might
 think that these methods need large datasets. It is valueable to
 note that a lot of data mining algorithms perform better in little 
quantity of data. For example, when considering losses due to losses, 
classification and regression trees exceeded multiple inferences in
 small samples~\cite{hid515-11}. 
As a second example, it was found that the use of 
shrinkage in the modeling of Bayesian 
structural equations produces less biased samples than the maximum
 likelihood estimation.

PCA and EFA are common data reduction methods in psychology, 
and EFA is often the first step in understanding the data
 dimension~\cite{hid515-11}. 
The EFA is used to dataset first, half of the dataset in general, 
CFA (confirmatory factor analysis) model
 is build based on the data left to devide 
the exploration with the validation of the data analysis. The 
algorithm is closed to cross-validation, however, psychologists 
do not validate the exact model~\cite{hid515-11}. 
In general, CFA is used to re-build the model, 
and the negligible factor load 
in the EFA is fixed at zero.

Closed to principal component analysis and exploratory factor 
analysis, cluster algorithms and finite mixture models
 are often used in psychology. 
Limited-mix models 
are increasingly used to search for groups with different data 
patterns or associations. In psychology, few influences are 
universal~\cite{hid515-11}. 
The limited mixed model is a method to 
search required effects. A problem of using the finite 
mixture model in psychology is that cross-validation is rarely used
 to assess the feasibility of the model. But, cross-validation
 has recently received more attention in hybrid 
modeling~\cite{hid515-11}.

Even though supervised learning methods are not commonly used in 
psychology, the majority of them can be ascribed to the less 
attention paid by those algorithms from scholars of psychological 
science methods. 
As the increase of data mining algorithms applied to 
the complexity of psychology dataset, this situation is 
slowly and surely changing. Specifically, the big data 
approaches with supervising learning will be applied more in 
psychology.

Due to our multivariate measurement and our fairly common vertical
 design, latent variable models (eg, confirmatory factor models, 
structural equation models [SEM]) are common in psychology. 
Mixing data mining algorithms with latent variable models is a 
significant step to increase the use of 
psychologists~\cite{hid515-11}. 
For example, researchcers can
develope SEM trees by combining SEM with classification and 
regression 
tree algorithms. In the SEM tree, the data is divided using a 
series of predictors, and the user-specified SEM fits into all
partition in the sets. The target is to figure out 
the predictors that 
maximize the fit of the model. Basically, it can be a method of 
automatically searching for a group of participants, where members
 of the same group are homogeneous about SEM, and members of 
different groups are heterogeneous about SEM~\cite{hid515-11}. 
For example, a SEM tree could be applied for
 finding groups with different groups with 
different measurement models.

Similarly, Jacobucci, Grimm and McArdle (2016) combined 
regularization methods commonly used in high-dimensional regression
 with SEMs to create regularized SEMs (RegSEM)~\cite{hid515-11}. 
RegSEM allows 
researchers to punish specific parameters in the SEM, resulting in
 a simpler, more replicable SEM. There are similar developments in
 the multi-level modeling framework. For example, Hajjem, 
Bellavance and Larocque (2011) and Sela and Simonoff (2012) 
combined
 a mixed effects model with a regression tree to create a 
mixed-effect regression tree~\cite{hid515-11}. 
These methods search efficiently in
 high-dimensional structured data with different level.

Although some effort has been made to help the algorithms more 
accessable to psychologists, but the difficulty of gaining 
attention, incomplete data, should be emphasized. 
In summary, a lot of data mining methods can not be applied without
 complete data. Moreover, different programs have different ways to
 handle the incomplete data~\cite{hid515-11}. 
Since most of the psychology datasets are incomplete and are 
usually not lost randomly, 
the model may produce biased results, or at 
least the result depends on the data used to deal with 
incompleteness. Therefore, an approach for researching in the 
future will 
greatly make the usefulness of these algorithms in psychology play 
a more important role, incorporating contemporary missing data methods 
(such as
 multiple attribution or comprehensive information estimates) into
 data mining plans~\cite{hid515-11}.

\section{Application of Certain Data Mining 
Algorithms in Psychology data set}

This section is based on a paper about applying certain data mining
 algorithms for extracting features and predicting results from a 
psychology data set~\cite{hid515-12}.

Human exercises are progressively influenced by advanced items and 
administrations~\cite{hid515-13}.
People utilize interpersonal interaction destinations to impart 
with informing applications, utilize online stages and Mastercards 
for installment, and transmit computerized media. Likewise, they 
are for all intents and purposes indistinguishable from wearable 
gadgets, for example, wellness 
trackers and advanced mobile phones. Progressively drenched in the
 advanced 
condition and depending on computerized gadgets implies that 
individuals
conduct, correspondence, geographic area and even physical 
status can be effectively recorded, bringing about countless 
computerized impression tests. These impressions incorporate web 
perusing 
logs, exchange records from on the web and disconnected markets, 
photographs 
what's more, recordings, GPS area logs, media playlists, voice and
 video 
call records, dialects utilized as a part of tweets or messages, 
and that's only the tip of the iceberg.

Late research shows that advanced impressions can be effectively 
utilized to consider critical mental results 
extending from digital footprint~\cite{hid515-14}, 
language~\cite{hid515-15}, and feelings~\cite{hid515-16}  
to social fit~\cite{hid515-17} and long range informal communication. 
Lamentably, an intrigue in concentrate computerized impressions, 
and in addition the vital aptitudes 
to do as such, are generally uncommon among social researchers. Thusly, 
such research is progressively surrendered to PC researchers and 
engineers, who regularly do not have the hypothetical foundation in
 social scienceand preparing in moral gauges relating to human 
subjects explore.

The following of this section will describe the steps of applying 
big data technique in psychology, most of the codes are written by R.

\subsection{Import the Dataset of Psychology in R~\cite{hid515-12}}

This subsection pay attention to how to import, 
store and preprocess huge dataset of digital footprints. 
A lot of large data sets can be obtained
 online for free, and can be obtained from companies and 
institutions that collect and store them. In a popular example, 
myPersonality.org database 1 stores scores of many 
psychological questionnaires and Facebook profile datasets 
for more 
than six million participantsta~\cite{hid515-11}. 
The Stanford University Web
Analytics Project website2 has a lot of datasets, such as
 social networking, Tweets, and product reviews. 
Besides, 
journal articles are accessable by publicly datasets 
recently~\cite{hid515-11}. 
 In addition, a lot of social medias (such as Twitter) 
have huge sums of data that could be recorded easily. 
Peer websites provide a collection relation to other interesting 
datasets. The following section 
describes the dataset, which is used as an example, used in this 
tutorial~\cite{hid515-11}. The sample 
data set was obtained from the myPersonality.org 
database~\cite{hid515-11}.

The data set used here contains psychodemographic profiles of 
110,728 Facebook users and their Facebook Likes. 
For the purpose of ease of management, the dataset is 
constrained users in 
United State~\cite{hid515-11}. 
The three files are introduce as follows:

1.\ user.csv: Storing the psychodemographic user profiles. It 
contains 
110,728 lines (not including line and column names) and 9 columns:
 anonymous user ID, gender (male for 0, female for 1), age, 
political opinion (Democrats for 0, and 1 for Republicans) 
as well as five scores in the 100-item International Personality 
Program Questionnaire to assess five factors (ie, openness, 
discretion, extroversion, agreeability, and neuroticism) 
personality models~\cite{hid515-18}~\cite{hid515-11}.

2.\ likes.csv: Contains anonymous ID and 1,580,284 Facebook like 
names. It contains two features: ID and name~\cite{hid515-11}.

3.\ users-likes.csv: Including relations between users and their 
likes. It has 10,612,326 rows of data and two 
features: User ID and Like ID. The existence of a similar user pair
 means that a given user has a corresponding Like on their 
profile~\cite{hid515-11}.

Using R to load the data files in Figure\ref{F:loading}:

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    users <- read.csv('users.csv')
    likes <- read.csv('likes.csv')
    ul <- read.csv('users-likes.csv')
\end{verbatim}
\end{footnotesize}
\caption{Loading data files. Copied from~\cite{hid515-12}}
\label{F:loading}
\end{figure}

Constructing a User–Footprint Matrix in Figure\ref{F:footprintmatrix}

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    ul$user_row <- match(ul$userid,users$userid)
    ul$like_row <- match(ul$likeid,likes$likeid)
\end{verbatim}
\end{footnotesize}
\caption{Constructing a User-Footprint Matrix. 
Copied from~\cite{hid515-12}}
\label{F:footprintmatrix}
\end{figure}


Useing the pointers to rows in the users and likes objects to 
construct a user–Like matrix with the sparseMatrix 
function from the Matrix
 library in Figure\ref{F:sparseMatrix}~\cite{hid515-12}:

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    require(Matrix)
    M <- sparseMatrix(i=ul$user_row, j=ul$like_row, x=1)
\end{verbatim}
\end{footnotesize}
\caption{Using sparseMatrix to build user-like matrix. 
Copied from~\cite{hid515-12}}
\label{F:sparseMatrix}
\end{figure}


The parameters i and j of the sparseMatrix function indicate the 
positions of the non-zero cells of the matrix (rows and columns, 
respectively). The parameter x represents the value of each cell. 
As mentioned earlier, Facebook users can only issue Like once, so 
we set all non-zero units to x equal to 1~\cite{hid515-12}.

Finally, the row-name of the similar user's Matrix M is set to 
contain the ID of each user, and the column name of M is set to 
contain each favorite name in 
Figure\ref{F:setting names}~\cite{hid515-12}.

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    rownames(M) <- users$userid
    colnames(M) <- likes$name
    dim(M)
\end{verbatim}
\end{footnotesize}
\caption{Setting matrix names based on datasets. 
Copied from~\cite{hid515-12}}
\label{F:setting names}
\end{figure}


\subsection{Trimming the User–Footprint Matrix~\cite{hid515-12}}

There is not a unique way for chosing the lowest 
frequency, which means under this frequency a certain user ought to be 
deleted, but the rules,which are discribed as follows, 
can be applied to make the decision:

1.\ Remove individual user and footstep instances from the 
dataset because they are not valuable for the extraction mode. 

2.\ Think about the limitation of hardware and computing time. 
Keeping too many data may cost more computational resources. 
On the
 other hand, deleting too many data points can significantly reduce
 the amount of information available for analysis. Therefore, it is
 recommended to plan and analyze a small number of randomly 
selected small samples of different sizes for approximating 
the relationship between required size of data. 
This not only tells you how much data you need to keep, 
but also speeds up writing code by reducing the time required for 
testing.

\subsection{Modifying the Matrix of User-Like~\cite{hid515-12}}

We use the following code to trim the user-like 
matrix in Figure\ref{F:Trimming}

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    repeat {
        i <- sum(dim(M))
        M <- M[rowSums(M) >= 50, colSums(M) >= 150]
        if (sum(dim(M)) == i) break
    }
\end{verbatim}
\end{footnotesize}
\caption{Trimming the User-Like Matrix.Copied from~\cite{hid515-12}}
\label{F:Trimming}
\end{figure}


This code uses repeated loops until interrupted by a command 
interrupt. 
Next, we are removing the users which are deleted from M in 
Figure\ref{F:delete}:

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    users <- users[match(rownames(M),
    users$userid),]
\end{verbatim}
\end{footnotesize}
\caption{Delete the users from users object. 
Copied from~\cite{hid515-12}}
\label{F:delete}
\end{figure}


\subsection{Extracting useful Features from the FootPrint 
Dataset~\cite{hid515-12}}

This subsection focuses on two approaches that represent two large 
families, extracting features from the user's footprint matrix: 
(a) Singular value decomposition, which represents a method based 
on feature decomposition, projecting a set of data into 
certain dimensions; (b) The potential Dirichlet distribution 
represents a cluster analysis method~\cite{hid515-12}. 
The main advantages of LDA 
and other cluster analysis methods are easy to explain. However, 
they are generally computationally expensive and can only handle 
non-negative data~\cite{hid515-12}. 
The main 
advantages of the singular value decomposition based on internal 
decomposition and many other methods are its simplicity and speed 
of calculation. Therefore, they are often used to develop 
predictive models. Compared with LDA, the method based on 
intrinsic decomposition can also be applied to data sets containing
 negative data points~\cite{hid515-12}.

Reducing the dimensionality of the data (or extracting the cluster)
 has many advantages. 
Firstly, in the case of large data sets, there
 are usually more variables than users. In this case, reducing the
 dimension is very important because most statistical analysis 
requires more users than variables (preferably more). Second, 
althought the number of users exceeds the number of variables, 
further 
reductions can reduce the risk of overfitting and may increase the
 statistical power of the results. Third, decreasing dimensions by 
eliminating multiple collinearity and redundancy in the data group
 related variables into a dimension or cluster. Fourth, a few 
 dimensions or clusters containing data are easier to 
interpret than hundreds or thousands of individual variables. 
Finally, reducing the dimensions reduces the computational time and
 memory required for further analysis~\cite{hid515-12}.

\subsection{Deciding How many Dimensions or Clusters to 
Extract~\cite{hid515-12}}

One of the significant ideas for reducing the dimension is 
to select correct sums of dimensions (in k) to be
 extracted. However, there is certain correct method to do that. 
Moreover, the expected value of k relies on the intended 
application~\cite{hid515-12}. 
If the target is to have deep explaination from the data, several
 dimensions or clusters may be easier to interpret and 
visualize~\cite{hid515-12}. 
Otherwise, if the goal is to generate a prediction model, more
 dimensions will retain more information from the 
original matrix to achieve more accurate 
predictions. However, if k is too high, 
the dimension reduction can not contribute as discussed before, 
and the accuracy of prediction will decrease instead of 
increasing~\cite{hid515-12}. 
SVD and LDA 
are going to be discussed in details and examples to chose k 
properly will be given in the following subsection.

\subsection{Algorithm of Singular Value Decomposition~\cite{hid515-12}}

SVD is a popular dimensionality reduction technique that is widely
 used in various environments, including computational social 
sciences, machine learning, signal processing, natural language 
processing, and computer vision~\cite{hid515-12}. 
Psychologists are usually more
 used to this method: Principal Component Analysis (PCA) as we 
discussed in the above section.
 In fact, PCA can be considered as a special case of SVD: SVD is 
performed on the centered matrix to generate V and $\sum$, 
which is equivalent
 to the feature vector and square root of the feature values 
generated by PCA~\cite{hid515-12}. 

SVD denotes a matrix, which is given before, of the products of 
three matrices 
(m rows
 and n columns): The matrix U containing the left 
singular vector contains a non-negative square diagonal matrix 
(size k); the matrix V (size nk) contains the correct singular 
vector, where k is the dimension which the experts chooses to 
extract~\cite{hid515-12}. 

\subsection{Centering the data~\cite{hid515-12}}

Before performing SVD to improve the interpretability of the SVD 
dimension, the data is usually centered (ie, the entries in the 
matrix are reduced column by column). This is because the first SVD
 dimension extracted from non-central data is closely related to 
the frequency of objects in rows and columns.11 Since the remaining
 dimensions must be orthogonal to the first dimension dimension, 
the resulting SVD dimension may not be well represented 
data~\cite{hid515-12}.

\subsection{Calculating SVD~\cite{hid515-12}}

The majority programming languages offer ready-made packages to 
calculate SVD: the svd function of Python's SciPy library, the 
feature library in C, the PROPACK library in Matlab, and the 
irlba package in R~\cite{hid515-12}. 
For the matrices which are sparse and large, it is 
recommended to apply SVD sparse variants acessable in the majority
 of the packages above. 
You may use the various R functions, Matlab, and other languages 
for rotating the factors.

\subsection{Latent Dirichlet Allocation~\cite{hid515-12}}

LDA assumes that each users and each footstep in the matrix (in the
 case of LDA, the user is referred to as a document, and the 
footprint as a word) belongs to a group of k clusters (known as 
topics) with a certain probability. For matrices of size m rows and
 n columns, the LDA generates a matrix of size mk, describing the 
probability that each user belongs to each cluster, and a matrix of
 size kn that describes the probability that each cluster belongs 
to each cluster~\cite{hid515-12}. 

\subsection{Choosing the number of k~\cite{hid515-12}}

As with other methods, there is no single or simple way to select 
the correct number k of LDA clusters to extract~\cite{hid515-12}. 
Common methods use
 the log-likelihood estimate of the model and can plot the number k
 of clusters extracted. This requires the generation of several 
models for different k values. Generally, for a lower range of k, 
the log-likelihood grows rapidly, flattens at higher k-values, and
 may begin to decrease once the number of clusters becomes very 
large. Choosing a marker that quickly increases the log-likelihood
 at the end of k usually explains these topics well. Larger 
k-values generally provide better predictive power. Alternatively,
 the interpretability of the topic may be supported by applying a 
hierarchical LDA~\cite{hid515-12}.

\subsection{Calculating LDA~\cite{hid515-12}}

The majority of languages provide ready-made LDA implementations: 
R theme model package, Pythons scikit-learn package, 
GibbsLDA++ for C++, and theme modeling toolbox for Matlab.

\subsection{Reduce the Dimensions of the
User–Like Matrix With SVD and LDA~\cite{hid515-12}}

The dimensions are reduced by the code in the Figure\ref{F:reduce}

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    set.seed(seed = 68)
    library(irlba)
    Msvd <- irlba(M, nv = 5)
    u <- Msvd$u
    v <- Msvd$v
\end{verbatim}
\end{footnotesize}
\caption{Reducing the dimesionality. Copied from~\cite{hid515-12}}
\label{F:reduce}
\end{figure}


The following code produces Vrot and Urot, the varimaxrotated
equivalents of matrices U and V in Figure\ref{F:produce}~\cite{hid515-12}.

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    v_rot <- unclass(varimax(Msvd$v)$loadings)
    u_rot <- as.matrix(M %*% v_rot)
\end{verbatim}
\end{footnotesize}
\caption{Produce Vrot and Urot. Copied from~\cite{hid515-12}}
\label{F:produce}
\end{figure}


As the analysing SVD case, seed 68 was used as the preset
 R generator of random number to make sure that the results 
presented below and computed by the readers are nor 
different~\cite{hid515-12}:

We use the code in Figure\ref{F:setting68}

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    library(topicmodels)
    Mlda <- LDA(M, k = 5, 
                control = 
                list(alpha = 10, delta = .1, seed = 68), 
                method = 'Gibbs')
    gamma <- Mlda@gamma
    beta <- exp(Mlda@beta)
\end{verbatim}
\end{footnotesize}
\caption{Preset the seed to 68. Copied from~\cite{hid515-12}}
\label{F:setting68}
\end{figure}


Consider expanding the test scope of ks, but note that the code 
below might run for a long time in Figure\ref{F:expanding}:

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    lg <- list()
    for (i in 2:5) {
        Mlda <- LDA(M, k = i, control =
        list(alpha = 10, delta = .1, seed = 68),method = 'Gibbs')
        lg[[i]] <- logLik(Mlda)
    }
    plot(2:5, unlist(lg))
\end{verbatim}
\end{footnotesize}
\caption{Expanding the test scope of ks. Copied from~\cite{hid515-12}}
\label{F:expanding}
\end{figure}


\subsection{Explaining the Dimensions and Clusters~\cite{hid515-12}}

This subsection focuses on explaining the dimensions and clustering
 extracted 
from the user's footprint matrix. This explanation is certainly not
 trivial, but it may provide important insights~\cite{hid515-12}. 
Several main
psychology models and theories are derived from different 
researches than those provided in this paper. For example, the 
lexical assumption assumes that significant interpersonal 
psychology differences change to be something as language. 
This concept
 is an important basement for many personal psychology and is
 applied to develop main personality structure, like the HEXACO or Big
 Five~\cite{hid515-12}. 
Expected interpersonal psychological differences and other 
psychological phenomena can also reflect the reasonable digital 
footprint in the model.

A few strategies may be used for explaining the charactors of 
patterns extracted from huge datasets. At the beginning, 
you can do exploration on the 
footprints that are relevant to the dimension which is given.
 The method is used for explaining the dimension and clustering 
extracted from the user movie Matrix X in the extracting schema 
part from the big dataset. 
Then, you may examine the 
relationship between dimensions and clusters as well as some of 
the user's known properties (for example, demographic information,
 psychological characteristics, responses to individual questions 
on the psychological questionnaire, etc.). Finally, you can 
explore the interpretation of dimensions or clusters by exploring 
them a hierarchy that spans different k levels~\cite{hid515-12}. 
The equivalent of LDA is provided by graded LDA.

\subsection{Interpretation of Clusters and Dimensions~\cite{hid515-12}}

Figure\ref{F:interpreting} shows the code:

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    cor(gamma, users[,-1], use = 'pairwise')
\end{verbatim}
\end{footnotesize}
\caption{Interpreting Clusters and Dimensions. 
Copied from~\cite{hid515-12}}
\label{F:interpreting}
\end{figure}


LDA1 members were negatively correlated with age, positively 
correlated with gender (male 0, female 1), political opinion 
(democrats 0, Republicans 1), indicating that this group was mainly
 reserved for young people. Female. Cluster LDA2 is similar but has
 nothing to do with gender. The group LDA3 contains elderly, 
liberal and politically free men. Compared with other groups, the 
correlation of LDA4 with the psychological population 
characteristics included in this sample was low. The last cluster 
LDA5 has the closest relationship with the personality dimension. 
Emotionally stable, happy, outgoing, responsible, conservative 
(including personality and political views), older and female users.

The quality of the connection amongst preferences and bunches is 
put away in R protest test. And the accompanying codes may be 
utilized for extricating the main ten Likes that are most intently 
connected with each cluster~\cite{hid515-12}:

The code is in the Figure\ref{F:extricate}.

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    top <- list()
    for (i in 1:5) {
        f <- order(beta[i,])
        temp <- tail(f, n = 10)
        top[[i]] <- colnames(M)[temp]
    }
    top
\end{verbatim}
\end{footnotesize}
\caption{Extricating the main 10 Likes. Copied from~\cite{hid515-12}}
\label{F:extricate}
\end{figure}


\subsection{Using the SVD dimensions~\cite{hid515-12}}

Use the following code in Figure\ref{F:obtain}to 
obtain the correlation between the maximum 
likelihood rotated SVD dimension (Urot) and the user score for the
 mental demographic user function:

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    cor(u_rot, users[,-1], use = 'pairwise')
\end{verbatim}
\end{footnotesize}
\caption{Obtaining the correlation. Copied from~\cite{hid515-12}}
\label{F:obtain}
\end{figure}


This loop previously applied for extracting a loop of the most 
representative LDA cluster can be used for extracting the likes 
of the ten highest scores and ten lowest scores in the SVD 
dimension~\cite{hid515-12}.
 For ranking likes based on the score of the SVD dimension, 
apply the command in Figure\ref{F:rank}:

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    f <- order(v_rot[,i])
\end{verbatim}
\end{footnotesize}
\caption{Ranking the likes based on the socre. 
Copied from~\cite{hid515-12}}
\label{F:rank}
\end{figure}


To extract a favorite index with extreme scores, using the tail 
and head functions in Figure\ref{F:extractindex} to provide the n last or 
first elements of the 
object~\cite{hid515-12}:

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    colnames(M)[tail(f, n = 10)]
    colnames(M)[head(f, n = 10)]
\end{verbatim}
\end{footnotesize}
\caption{Extract a favorite index. Copied from~\cite{hid515-12}}
\label{F:extractindex}
\end{figure}


\subsection{Predicting the Real-Life Outcomes~\cite{hid515-12}}

This part focuses on generating a prediction model based on 
dimensions which are extracted from the footprint matrix.
 14 A large number of methods may be applied to build predictive 
models based on the datasets, from relatively complex methods to 
simpler methods~\cite{hid515-12}. 
In fact, it is wise to begin using a 
simple forecasting method. Besides, according to the 
experience, simple models often offer similar accuracy for more complex 
methods - and are easier to interpret~\cite{hid515-12}. 
Finally, using simple methods can reduce the
 risk of errors, maximize the transparency of the method, and 
promote the reproducibility of results. Here, we use linear
 and logistic regression, which is known by most of psychologists.

\subsection{Applying the Cross-Validation~\cite{hid515-12}}

For reducing the variability of results, multiple rounds of 
cross-validation are often performed using different data 
partitions~\cite{hid515-12}, such as  
K-fold cross-validation divides the data 
into k subsets of the same size. 
And models are generated with training subsets and verified on 
the other test subset. The process mentioned above will be repeated
 k times for every subset and the mean accuracy is caculated base 
on all results~\cite{hid515-12}.

Using R, the cross-validation is easily to be done, 
such as applying loops, but there are also 
well-specified libraries. Common R libraries that support 
cross-validation include boot and caret~\cite{hid515-12}. 
In Python language, the scikit-learn package contains the 
cross-validation module.

\subsection{Predicting the Real-Life Outcomes of
Facebook Likes~\cite{hid515-12}}

The following code generates a vector collapse with a length equal
 to the number of users, consisting of randomly selected numbers 
ranging from 1 to 10 using the code in 
Figure\ref{F:select}~\cite{hid515-12}:

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    folds <- sample(1:10, size = nrow(users), replace = T)
\end{verbatim}
\end{footnotesize}
\caption{Selecting 10 users samples. Copied from~\cite{hid515-12}}
\label{F:select}
\end{figure}


Next, users with a folding value equal to 1 are set as the 
test dataset and the other data points are set as the training 
dataset. The code in Figure\ref{F:logic} generates the 
logic vector15 test, if 
the object is collapsed to 1, the test result is TRUE, otherwise 
returns FALSE:

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    test <- folds == 1
\end{verbatim}
\end{footnotesize}
\caption{Seperate the dataset to testing and training. 
Copied from~\cite{hid515-12}}
\label{F:logic}
\end{figure}


Logical vectors can be used to extract the desired elements from R
 objects. In addition, the true or false factors
 which is of the logic vector can be easily represented with the 
logical operators rather than [!], 
so instead of creating a separate 
logical vector, vector indicating the degree of membership in the 
training subset, you can simply use the test [!] to 
test~\cite{hid515-12}. SVD scores of 
Preferences are varimax-turned and used to register varimax-pivoted
client SVD scores for the whole example.
Therefore, the SVD scores of the users in the test dataset depend
 on the analysis performed only on the training dataset, which 
preserves independence of the results generated by the 
training and test dataset~\cite{hid515-12}:

We use the code in Figure\ref{F:scores} to get the SVD scores.

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    Msvd <- irlba(M[!test,], nv = 50)
    v_rot <- unclass(varimax(Msvd$v)$loadings)
    u_rot <- as.data.frame(as.matrix(M %*% v_rot))
\end{verbatim}
\end{footnotesize}
\caption{to get the SVD scores. Copied from~\cite{hid515-12}}
\label{F:scores}
\end{figure}


Finally, it is pointed out that all the independent variables 
should be used to predict the variable ope in the object of user.

The code in Figure\ref{F:predict} is applied to predict.

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    fit_o <- glm(users$ope, data = u_rot, subset = !test)
    fit_g <- glm(users$gender, 
    data = u_rot, subset = !test, family = 'binomial')
\end{verbatim}
\end{footnotesize}
\caption{Predict to variable. Copied from~\cite{hid515-12}}
\label{F:predict}
\end{figure}


Generate predictions using the models (fito and fitg) developed in
 the previous step and the user's maximum rotation SVD score 
(urot [test,]) in the test subset in the Figure\ref{F:generate}:

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    pred_o <- predict(fit_o, u_rot[test,])
    pred_g <- predict(fit_g, u_rot[test,], type = 'response')
\end{verbatim}
\end{footnotesize}
\caption{Generating the prediction. Copied from~\cite{hid515-12}}
\label{F:generate}
\end{figure}

The accuracy of linear regression model may be easily 
shown as Pearsonal product moments correlation in Figure\ref{F:accuracy}:

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    cor(users$ope[test], pred_o)
\end{verbatim}
\end{footnotesize}
\caption{Testing the accuracy. Copied from~\cite{hid515-12}}
\label{F:accuracy}
\end{figure}


Now, the prediction performance is estimated, which is based 
on the SVD dimensions of k = 50. Then, we start to consider the 
connection between
 cross-validation accuracy of prediction and SVD dimension. 
In code snippet which will be shown below, the k values of 
the dataset ks is difned first, which we are going to test. 
It contains values from 2 to 10, 15,
 20, 30, 40 and 50~\cite{hid515-12}. 
After that, we generate the empty list rs to save the values. 
Then a for loop needs to be started, run the code that is 
surrounded by braces, and change the value of i to a continuation 
element stored in ks~\cite{hid515-12}. 
This code in parentheses is closed to the code applied previously,
 however, the first of the k = 50 SVD dimensions is used. This 
result is stored as a new member in the 
list rs. The R's random number generator is fixed
 again for ensuring that the results in the paper match the 
reader's results as shown
 in Figure\ref{F:Fixing R's random number generator}:


\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    set.seed(seed = 68)
    ks<-c(2:10,15,20,30,40,50)
    rs <- list()
    for (i in ks){
        v_rot <- unclass(varimax(Msvd$v[,1:i])$loadings)
        u_rot <- as.data.frame(as.matrix(M%*%v_rot))
        fit_o <- glm(users$ope, data = u_rot, subset = !test)
        pred_o <- predict(fit_o, u_rot[test,])
        rs[[as.character(i)]] <- cor(users$ope[test], pred_o)
    }
\end{verbatim}
\end{footnotesize}
\caption{Fixing R's random number generator. 
Copied from~\cite{hid515-12}}
\label{F:Fixing R's random number generator}
\end{figure}


Finally, the open cross-validation was calculated forecast for the 
whole data samples. For doing the cross-validation,  a model was 
constructed and used this data 10 
times to calculate the predicted value. First, a vector was created
predo to save the result of predicting the whole data samples. 
It contains a lot of 
missing values, which is shown as NA and contains members as many
as the users. 
Then, the for loop is used to re-run the previously described
 code with choosing a continuous cross-validation fold. After the 
loop is completed, the correlation was 
caculated between the actual open score and the predicted open 
score for all users in the sample~\cite{hid515-12} 
in Figure\ref{F:cross1}:

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    set.seed(seed = 68)
    pred_o <- rep(NA, n = nrow(users))
    for (i in 1:10){
        test <- folds == i
        Msvd <- irlba(M[!test,], nv = 50)
        v_rot <- unclass(varimax(Msvd$v)$loadings)
        u_rot <- as.data.frame(as.matrix(M %*% v_rot))
        fit_o <- glm(users$ope, data = u_rot, subset = !test)
        pred_o[test] <- predict(fit_o, u_rot[test,])
    }
    cor(users$ope, pred_o)
\end{verbatim}
\end{footnotesize}
\caption{Calculate the Cross-validation. Copied from~\cite{hid515-12}}
\label{F:cross1}
\end{figure}


The openness prediction accuracy, which is estimated on the entire
sample, is r = 0.44~\cite{hid515-12}.

The accompanying case of code demonstrates to 
build up LDA show on the preparation dataset and afterward 
utilize it to 
appraise LDA group scores for all clients utilizing the capacity 
back~\cite{hid515-12}.
Use a for loop to wrap it in Figure\ref{F:wrap}, 
closed to the above code applied to produce SVD-based prediction:

\begin{figure}[htb]
\begin{footnotesize}
\begin{verbatim}
    Mlda <- LDA(M[!test,], 
                control = list(alpha = 1, delta = .1, seed = 68),
                k = 50, method = 'Gibbs')
    temp<-posterior(Mlda, M)
    gamma <- as.data.frame(temp$topics)
\end{verbatim}
\end{footnotesize}
\caption{Wrapping the LDA. Copied from~\cite{hid515-12}}
\label{F:wrap}
\end{figure}

\section{Social concerns of applying Big Data 
technology in Psychology}

As discussed above, applying big data technique in the psychology 
field will generate a lot of advantage, such as the survey of 
psychology can be done with less bias, the data for researching the
the psychology can be collected more efficiently and stored more 
safely, and new types of data can be used for research, such as the
the interact data between the people and websites searching. 
There are huge amount of bennifits to use big data technique in 
psychology field, nevertheless, every thing has the other side. Big
data technique also generates new problems when it is applied in 
psychology. 

Obviously, the first big risk is about the privacy. Big data 
technique collects, stores and is used for research without human 
awareness. When a person is searching on internet, the interactive
data is generated, such as which websites do the person visit a 
lot, which kind of key words do the person use the searching 
engines most, what is the personal habbits of clicking the mouse on 
the websites, etc. All this information is generated with few self
awareness, and some people do not want this information be known by
others, but with big data technique, it is collected and even used 
for research, therefore, privacy should be the first issue to be 
considered carefully when applying the big data technique in 
psychology. 

Secondly, safety is an even more serious risk. As discussed above, 
information can be collected easily with the big data technique. 
Some of those informations are purely privacy issue, but some of 
those are related to safety problem. For example, certain datasets 
are the account information of users of platforms as Facebook, Yelp
, Twitter and Amazon. There are not just interactive data in the 
account dataset. Name, gender, age, address and even bank account
are the usual features in a registered account. With big data 
technique, these informations are easily to be collected and even
packed to transport to different data scientists to analyze, 
therefore, all the information related to safety is easy to be 
acessed. With bank account and address information, criminals can
commit crimes efficiently and accurately.

Thirdly, big data technology can mislead human behavior. Most of 
the big data algorithms are related to predict something based on 
the existing datasets. And the majority of the algorithms are based
on probability, without knowing the reason behind. Some prediction
has low accuracy displayed, so people don't believe this kind of 
prediction easily. In the other hand, the society tend to trust the
high accuracy predition results. However, because the reason behind
the prediction is not understood, the bias of the datasets can not 
be remove clearly. So, some prediction might be suitable for 
certain areas of people, and if the people in a totally different 
area trust deeply in those predictions, then their behaviors are
mislead easily. Unless a standard is developed in the future to 
clarify that the bias of the datasets is reduced to acceptable 
level or even totally cleaned, the predictions of big data 
technique should considered with carefulness, and some predictions
should even not released to the public.

The concerns listed above are some obvious and the most serious 
issues in the perspactive of this paper. As the discussion going 
on, 
more risks will occur to people's considerartion. But should the 
society give this outstanding new technology just because it will
cost new issue? Every new tech brings pros and cons to the society
at the time they were invented. People got terrified and excited 
at the same time. Therefore, whether this technology will bring us
progress or dangerous, depends on how people use it. Before 
applying it fully, we should consider it thoughtfully.


\section{Conclusion and Discussion}

Therefore, Big data is very likely to provide value to psychology.
 But, the using big data technique is still a risky career to
 ordinary psychology researchers.

How can psychology adapt to the field of big data or computing 
social sciences and other related fields? Psychology has begun to 
cover many areas like health, mental issue, depressing, behavioral
 issue, behavioral changes, working environment, study behavior 
and adjustment, and behavioral instict. Researchers are working on 
areas like health and people conditions with large datasets that
 contain thousands of people. Psychologists research that 
can connect these individual data with health, 
discovering patterns and connections among behavior and different 
result.

Obviously, big data stay in the society, no matter the psychology 
cooperates or not. This increasing field provides interested 
thinkers with a unique opportunity to address the confusing 
high tech challenges associated with 
saving, getting, researching, and justifying huge datasets. 
In turn, these mental models give
 non-psychological data (such as medical data related to 
health-related interventions; prosperity and depression related to
 financial investment behavior). The big data community and big 
data itself can jointly promote the development of psychological 
science.


\begin{acks}

  The authors would like to thank Dr.~Gregor~von~Laszewski for his
  support and suggestions to write this paper.

\end{acks}


\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

