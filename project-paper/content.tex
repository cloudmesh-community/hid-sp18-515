% status: 0
% chapter: TBD

\title{Paper: Big Data in Pychology}


\author{Qingyun Lin}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{Smith Reserch Center}
  \city{Bloomington} 
  \state{IN} 
  \postcode{47408}
}
\email{ql10@iu.edu}


% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{G. v. Laszewski}


\begin{abstract}
There is much discussion about the value of company big data. 
For example, Amazon will match your shopping and page views with 
other shoppers’ purchases and pageviews, and try to find people with 
similar interests. Then Amazon recommends buying products that people 
like, assuming you will like them.

Can big data be applied to answer the research community’s interest 
in psychology?

This paper will introduce what is pychology and what is big data 
technique simply, and why these two fields can be combined togethor, 
how can the society benefit from this combination, and introduce 
several combination examples and what is the future of this field.
\end{abstract}

\keywords{hid-sp18-515, big data, pychology, combination, future}


\maketitle

\section{Introduction}

Pychology problem is getting more and more attention in the morden 
society. And as the big data technology is such a heated topic today, 
the combination of this two fields should be well applied.

Big data is particularly good at dealing with issues that people may 
be reluctant to answer surveys. Often, the way people interact with 
computers reveals the interest they would not express in interviews 
or anonymous surveys.

Significant advances in computing technology, coupled with the 
proliferation of large digital networks and social media platforms, 
have produced an unimaginable amount of information about people. 
It is estimated that the amount of digital data currently present is 
in the thousands of exabytes or 10 to the 18th power of bytes.

This era of big data is likely to change the way psychologists observe 
human behavior. However, just as it creates new opportunities, access 
to a large amount of information also brings new challenges to research. 
Michael N. Jones of Indiana University Bloomington introduced the 
Big Data Project at the 2014 APS Annual Conference.

Each little piece of data is a trace of human behavior and offers 
us a potential clue to understanding basic psychological principles, 
said Jones. But we have to be able to put all those pieces together 
correctly to better understand the basic psychological principles 
that generated them\cite{editor00}.

The study of language development as one of Jones's own research interests 
is a good example of big data research. Collecting a large number of infant
samples from the natural environment is time-consuming and often results 
in small sample sizes. The test theory of the way children learn 
languages is a long time.

Big data can help speed up the process. As a proof of concept, Jones 
showed that based on associative learning theory, more than 100,000 
words from natural language can be entered into a computer model - the 
idea is that children group words together according to how often 
they use them near other words. Jones said that as the analysis progressed, 
the model did realize that the relationship between computers and data 
and word classes was closer than computers and obsolete.

Jones said that in the end, a similar analysis can be used to study 
the theory of connected learning in direct child conversation samples. 
As long as they have enough data to continue, these models are very 
good at learning from noise, he said\cite{editor00}.

Tanzeem Choudhury, an information scientist at Cornell University, 
said that big data may help researchers reach the point where they 
can collect behavioral information without sampling human participants. 
Technologies such as smart phones and wearable sensors can collect 
information about physical activity, social interaction, geographic 
location, and more\cite{editor00}.

The result of this data collection is that it is effective for users; 
it does not require their time or effort, and it greatly reduces 
self-reporting errors.

We can continuously get measurements of behavior without bugging 
people to fill out surveys, said Choudhury. We can potentially get 
continuous measurement without actually having to engage users all 
the time and rely on their self-input\cite{editor00}.

Chowdhury participated in some of these projects. StressSense can track 
daytime stress and help them avoid anxiety. For example, MyBehavior uses 
physical activity patterns to suggest ways of maintaining the shape, for 
example, working more frequently along lines that the user seems to like. 
MoodRhythm allows patients with bipolar disorder to monitor sleep and social 
interactions to maintain emotional and energy balance, which is a major 
improvement in pen and daily behavioral tracking\cite{editor00}. 

Big data has produced positive changes in the online search field. 
Susan T.Dumais of Microsoft Research Inc said that the billions of 
Internet searches that happen each day leave the behavior log that 
analysts use to improve search engines. Without such a large record, 
sites such as Google and Bing will never be able to use 2.4 words in 
an average Internet search and turn it into useful 
content\cite{editor00}.

Behavioral logs allow us to characterize, with a richness and fidelity 
that we’ve never had before, what it is people are trying to do with
the tools and systems they’re interacting with, said Dumais\cite{editor00}.

By mining behavior logs, analysts can create personalized algorithms 
to improve the user's search experience. For example, if Dumais searches 
for sigir, she may need the homepage of the Information Search Special 
Interest Group (SIGIR). If Little Stuart Bowen performs the same search, 
he may need the site to hold his position: Special Inspector General 
for Reconstruction of Iraq (also known as SIGIR).

In other words, the system can know that words and acronyms do not always 
predict the best way users want to search. Modeling the search in a way 
that considers the context of the published query is very important for 
improving Web search. Previous search activity is important, and the 
location and time of the query are also important. For example, the 
search for Spring US Open may refer to a golf ball, while the same 
search in late summer may refer to tennis.

Before you were able to collect Big Data, the person who spoke loudest, 
or the highest-paid person’s opinion, would dominate, said Dumais. Now 
the data, especially when derived from carefully controlled Web-scale 
experiments, dominates.\cite{editor00}

Big Data also allows researchers to rethink past issues in new ways, 
said Brian M. D'Onofrio, an APS researcher at Indiana University Bloomington. 
In particular, he pointed out that researchers should consider re-adjusting 
data that may be collected for other reasons. Reusing big data samples can 
help researchers generate insights. Traditional samples can't achieve the 
statistical power that many labs lack. This is a big challenge for psychology 
to improve its methods and replication process.

With Big Data, it gives you the opportunity to use several different 
types of quasi-experimental designs, to help rule out alternative 
explanations, D’Onofrio said\cite{editor00}.

D'Onofrio and his collaborators recently rearranged the millions of 
personal records compiled by Sweden in order to challenge the traditional 
notion that smoking during pregnancy directly leads to subsequent negative 
behavioral outcomes such as crime. In one study, the researchers analyzed 
50,000 siblings whose mothers smoked but did not smoke during pregnancy. 
They determined the family context factor, not smoking during pregnancy, 
related to criminal convictions. This recognition can greatly improve 
interventions: 
In this case, allowing women to quit smoking should only be part of 
broader social services\cite{editor00}.

Tal Yarkoni of the University of Texas at Austin says big data can 
even help psychologists study research. Yarkoni and others recently 
developed Neurosynth, an online program for analyzing a large number 
of fMRI data to guide users to topics of interest. Yarkoni said that 
so far, Neurosynth has synthesized studies from more than 9,000 
neuroimaging studies and about 300,000 brain activations\cite{editor00}.

One of Neurosynth's main goals is to distinguish brain activity that 
is always associated with a specific psychological process but not 
specific, and high-probability brain activity that means having a 
specific mental process. For example, painful physical stimulation 
may continue to produce patterns of brain activity, but this pattern 
of activity does not necessarily mean the presence of pain; other 
mental states may produce similar patterns. Infer mental processes 
from observed brain activity, this process is called reverse 
reasoning, it is difficult to do in a single neuroimaging study\cite{editor00}.

Neurosynth overturned reasoning by accumulating a large number of 
images and research data in one place. For example, even if some 
active brain regions overlap in these three cases, the database can 
help researchers find areas of the brain that are particularly relevant 
to pain, not working memory or emotions. Tests have shown that, in many 
cases, Neurosynth's performance and analysis are done manually by 
screening research literature - saving hundreds of hours of research 
time compared to just pressing a button, Yarkoni said\cite{editor00}.

That’s the long-term goal, Yarkoni said. To do this in a quantitative, 
automated way instead of a manual, qualitative way\cite{editor00}.

The long term goal is the theme of the plan, because big data is 
still the existence of science. Not everyone believes that it will 
bring a paradigm shift. Even if big data does change statistical analysis, 
it cannot replace Strong behavioral theory or experiment. But in terms of 
improving these theories or strengthening big data for these experiments, 
researchers cannot ignore it.

\section{Simple Introduction of Psychology}

Psychology is the science of behavior and mind, including conscious 
and unconscious phenomena, as well as feelings and thoughts. This is 
a wide range of subjects with diverse interests. When they are 
together, they try to understand the sudden nature of the brain and 
all the various post-show phenomena they exhibit. As a social science,
 it aims to understand individuals and groups by establishing general
 principles and studying specific cases\cite{editor01}.

In this area, professional practitioners or researchers are called 
psychologists and can be classified as social, behavioral, or 
cognitive scientists. Psychologists try to understand the role of 
mental function in personal and social behavior, and also explore 
the physiological and physiological processes behind cognitive 
function and behavior\cite{editor01}.

Psychologists explore behavioral and psychological processes, 
including perception, cognition, attention, emotion (emotion), 
intelligence, phenomenology, con lation, brain function and 
personality. This extends to human interactions such as interpersonal
 relationships, including psychological resilience, family resilience,
 and other areas. Psychologists in different directions also consider
 unconscious thoughts.\cite{editor02} Psychologists use empirical 
methods to infer causality and correlations between psychosocial variables. 
In addition, or on the contrary, using experience and deduction methods,
 some clinical and counseling psychologists sometimes rely on 
symbolic explanations and other inductive techniques. Psychology is 
described as a central science whose psychological research results 
are linked to research and social sciences, natural sciences, 
medicine, humanities, and philosophy.

Although psychology knowledge is often used to assess and treat 
mental health problems, it also points to understanding and solving 
problems in several areas of human activity. Many people's psychology
 is ultimately for the benefit of society. Most psychologists are 
involved in clinical, counseling or some kind of treatment and 
practice in the school environment. Many people usually work in 
university psychology or other academic environments (such as medical
 schools, hospitals) and engage in extensive scientific research on 
psychological processes and behaviors. Some people are employed in 
industrial and organizational environments or other fields such as 
human development and aging, sports, health and media, as well as 
legal investigations and other aspects of the law.

\section{Big data}

Big data is a data set. It has a large number and complexity. 
Traditional data processing applications are not enough to cope with 
this data. Big data challenges include capturing data, data storage, 
data analysis, search, sharing, transmission, visualization, querying,
 updating, information privacy, and data sources. There are a lot of 
concepts related to Big Data: There are 3 conceptual volumes, breeds,
 and speeds. Other concepts that were later triggered by big data are
 accuracy (ie, how much noise in the data) and value\cite{editor03}.

Recently, the term big data tends to refer to the use of predictive 
analytics, user behavior analysis, or some other advanced data 
analytics methods that extract value from data and are rarely used for
 data sets of a specific size. There is no doubt that the amount of 
data available today is indeed large, but this is not the most 
relevant feature of this new data ecosystem. Dataset analysis can 
find new relevance to discover commercial trends and prevent 
diseases, Combat crime, etc.\cite{editor04}.  Scientists, corporate 
executives, medicine, advertising and government practitioners often 
encounter difficulties in large-scale data collections including Internet 
search, financial technology, city information and business 
information. Scientists have encountered limitations in the work of 
electronic sciences, including meteorology, genomics, linkomics, 
complex physics simulations, biology and environmental studies\cite{editor05}.

Datasets are growing rapidly - partly because they are increasingly 
using portable and mass information-aware IoT devices (such as mobile
 devices, antennas (remote sensing), software logs, cameras, 
microphones, radio frequency identification (RFID) readers) Since 
the 1980s, the amount of information stored by the world's technology
 per capita has roughly doubled every 40 months; As of 2012, it 
produced 2.5 exabytes per day. Data According to IDC, the global 
data volume will increase from 4.4 zettabytes in 2013 to 2020 to 44 
zeta. By 2025, IDC predicts 163 megabytes of data. For large 
companies, one issue is to determine who should have big data 
initiatives that affect the entire organization\cite{editor03}.

Relational database management systems and desktop statistics, 
visualization of data packets is often difficult to handle big data. 
This work may require a lot of parallel software running on dozens, 
hundreds, even thousands of servers. What is big data depends on the 
functionality of users and their tools, and the expansion of big data
 as a moving target. For some organizations, facing hundreds of 
gigabytes of data for the first time may require reconsidering data 
management options, and for other organizations, it may take tens or 
hundreds of terabytes before data size becomes an important 
consideration\cite{editor03}.

Data Mining Algorithms

Data mining is an iterative process intended to discover patterns
in large quantities of data. These patterns are then used to aid 
in the answering of questions related to this data. Additionally,
data mining is capable of assigning new meaning to data, thus 
creating information that was not originally present or obvious. 
While data mining does not solve problems in and of itself, it 
does answer questions about the data. These answers can lead to 
more data mining problems, and occasionally to a solution to the 
original problem. Data mining is done in nine general steps. The 
steps are: problem statement, get data, enrich data, clean data, 
integrate data, transform data, mine, validate or interpret 
results, and make the results actionable. These will be discussed 
in greater depth below\cite{editor06}.

Two kinds of problems that often arise in data mining are clustering
and classification\cite{editor06}. Clustering involves grouping objects
based on the similarities of their attributes. This can be useful
for applications such as image processing or online shopping
recommendations. On the other hand, classification is assigning
one of a set of definite labels to an object. Clustering is
an example of unsupervised learning, while classification is
an example of supervised learning. Many approaches to data
mining use a loss function to make their predictions. A loss
function is a mapping from some event to a real number based
on the cost of that event happening. This cost does not have
to be strictly monetary; it could be time, lives, or infinite other
types of costs. It is important to note that, even once data
mining on a given project is complete, it does not definitively
tell the data miner what the next steps are. To obtain that
information, it is necessary to analyze the results of the data
mining.

There are ten commonly used algorithms in data mining.
The followings are based on Top 10 algorithms in data mining\cite{editor07}.
These are:

1. C4.5  This algorithm is a descendant of the older ID3 and CLS algorithms.
Like these, it uses decision trees to classify data. The
tree is grown in a divide-and-conquer manner by recursively
partitioning the data and then assigning a leaf to a class label
once the data cannot be partitioned or subsequent partitions
will not aid in the prediction. This algorithm can handle either
numeric values or nominal values, and the nature of the data
affects how the partitioning is done. Once this is complete,
the tree is pruned to prevent overfitting. Unfortunately, C4.5’s
generation of rulesets is very CPU and memory intensive,
taking 9715 seconds to analyze 100,000 samples.

2. K-means  This algorithm partitions data into a given number of clusters,
k. First, k centroids are chosen, and there are many different
ways to pick these centroids. One commonly used approach
is randomly sampling the data. Once this is complete, each
data point is assigned to the nearest centroid. Obviously, there
is no single way to decide ”nearness”; as such, the decision
of how to compute distance is extremely important for the
success of this algorithm. Next, the mean of every cluster
is computed, and this mean takes the place of the centroid.
The process is complete when the data assigned to a cluster
no longer changes. Unfortunately, there are numerous issues
with this algorithm. The most obvious of these is that setting
k can be difficult, and setting k poorly will dramatically affect
the algorithm’s performance. Additionally, it is sensitive to
outliers.

3. Support vector machines (SVM)  SVM are a robust and effective 
approach to classification.
SVM are best understood from the standpoint of a linearly
separable data set. In this case, the SVM finds a hyperplane
separating the data into two classes by maximizing the distance
from the line to the nearest points from either class. Of
course, data is almost never linearly separable, so part of the
SVM formulation is handling the case where some data is on
the wrong side of the line. Additionally, a kernel function
can be applied when the data is not linearly separable, which
is currently an area of active research. SVM struggled with
with slow performance, but this is also being actively solved.
This is done mostly by breaking the large optimization problem
into many smaller problems.

4. Apriori  This algorithm is intended to find frequent itemsets - that is
items that occur together frequently - through generating candidates.
First, it generates candidates for frequent itemsets of
a certain size k+1 from the itemsets of size k. Then it calculates
the support for each candidate to determine which of the
candidates is an actual frequent itemset. The candidates that
meet the minimum support are added to the frequent itemsets
of size k+1, and the process is repeated. Unfortunately, the
Apriori algorithm generates a large number of candidate sets
in some cases, namely when there are many frequent itemsets
or low minimum support. Additionally, the algorithm
must generate many candidates for itemsets with a large size.
For example 2100 candidates must be generated for frequent
itemsets of size 100.

5. Expectation-Maximization (EM)  The EM algorithm involves 
fitting normal mixture models
by maximum likelihood. This has numerous advantages, one
of which is that it allows the algorithm to model continuous
data well. It also allows for the modeling of latent variables,
or variables that aren’t explicit in the dataset. The algorithm
proceeds as one would expect: by finding expectation of the
log likelihood function of the conditional distribution of a
latent variable given the data and a current estimate of the
parameters of the distribution. Then it finds the parameter that
maximizes this value. The issue with this algorithm is that the
number of clusters must be chosen, and there are many ways
to accomplish that.

6. PageRank  Famously developed by Larry Page and Sergey Brin and used
by Google, PageRank ranks web pages by considering a link
to a page as a vote for that page. Then it ranks the pages
by how many ”votes” they get. However, the algorithm also
considers the importance of a page doing the voting, since
a higher-ranked page should have more clout in what pages
it deems important. It accomplishes this task by treating the
web as a directed graph and then counting inlinks as part of
a vertex’s indegree and the outlinks as a vertex’s outdegree.
The algorithm then simply iterates through the vertices until
the ranks do not change very much.

7. AdaBoost  AdaBoost is an ensemble learning system developed by Yoav
Freund and Robert Schapire. It is extremely accurate and
simple, with implementation possible in just ten lines of code.
First, this algorithm uses a simple learning algorithm to generate
class labels for the data. Then it uses the training data to
test this learner. It repeats this process T times, and then the
result is decided by weighted majority voting by the results of
the weak learners. Of course, this does not handle the multiclass
case. There is a variant of the algorithm for this case
that splits the decisions into a series of binary classification
problems. It is also interesting to note that this series of weak
learner is comparable in accuracy to a strong learner.

8. K-nearest neighbors  This algorithm involves finding k items 
in the data set that
are closest to a given object. Like k-means, this ”closeness”
is not defined, so it is necessary to choose a distance metric
carefully. For an unlabeled object, the object’s k nearest
neighbors are calculated, then those neighbors’ class labels
vote on the class of the object. One issue with this algorithm
is that it is very sensitive to the choice of k. Also, the majority
vote is not always a good way to decide the class of an object,
since some neighbors may be vastly closer than others. To
solve this, a distance-weighted voting system can be used.
K-nearest neighbors is well-suited for data in which an object
can have multiple labels.

9. Naive Bayes  Naive Bayes is a commonly used algorithm that, while usually
outperformed by more complex algorithms, can be relied
upon to provide good results in a wide variety of problems.
Naive Bayes simply calculates the conditional probability of
the features given the label and chooses the label with the
maximum probability. However, this means that the data must
be discretized in some way to ensure that the probability can
be computed. Additionally, this algorithm assumes that the
features are all independent. They rarely are independent, but
this does not make the algorithm perform poorly in practice.

10. CART  CART, standing for Classification and Regression Trees, was
developed by Leo Breiman, Jerome Friedman, Richard Olshen,
and Charles Stone. The algorithm builds a tree to maximal
size and then prunes it back recursively based on which
subtree contributes the least to the accuracy of the classification.
This accuracy is determined based on external test data or
cross validation. CART also supports class balancing, which
is an endeavor to make sure the pruning does not unfairly
take more occurrences of one class away. The importance of
an attribute is calculated by the sum of the improvement in
performance in all nodes where the attribute is split upon.

Big Data Technology

1. REST Representational State Transfer
REpresentational State Transfer (REST) is an architectural style 
that defines a set of constraints and attributes based on HTTP. 
Web services that conform to REST-style or REST-style Web 
services provide interoperability between computer systems on the 
Internet. RESTful web services allow the requesting system to 
access and manipulate the textual representation of web resources 
by using a uniform and predefined set of stateless operations. 
Other types of web services (such as SOAP web services) expose 
their own set of arbitrary operations\cite{editor08}.

The web resource is first defined on the World Wide Web as a 
document or file specified by its URL. However, today they have a 
more general and abstract definition, including anything that can 
be identified, named, processed, or processed in any way on the 
Internet. In a RESTful web service, a request for a resource URI 
triggers a response that may be in XML, HTML, JSON, or other 
format. The response can confirm that some changes have been made 
to the stored resources, and the response can provide hypertext 
links to other related resources or collections of resources. 
When using HTTP, the most common operations are GET, POST, PUT, 
DELETE, and other predefined CRUD HTTP methods\cite{editor08}.

By using stateless protocols and standard operations, REST 
systems are designed to achieve fast performance, reliability, 
and growth capabilities, even at runtime, by reusing manageable 
and updated components without affecting the entire system\cite{editor08}.

The transformation of representative countries was introduced and 
defined in Roy Fielding's doctoral dissertation in 2000. 
Fielding's paper explains the REST principles known as the HTTP 
Object Model since 1994 and is used to design the HTTP 1.1 and 
Uniform Resource Identifier (URI) standards. The term is intended 
to evoke a behavioral image of a well-designed web application: it
 is a web resource network (virtual state machine) where the user 
selects the link (eg / user / tom) in the application The progress
 of the program, as well as operations such as GET or DELETE 
(state transitions), cause the next resource (indicating the next 
state of the application) to be passed to the user for its use\cite{editor08}.

2. Swagger
Swagger is an open source software framework powered by a large 
ecosystem of tools that helps developers design, build, document, 
and use RESTful Web services. Although most users recognize Swagger
 through the Swagger UI tool, the Swagger toolset includes support
 for automatic documentation, code generation, and test case 
generation\cite{editor09}.

3. Docker
Docker is a computer program that performs operating system level 
virtualization, also known as containerization. It was developed by
 Docker, Inc. Docker is primarily developed for Linux. It uses 
Linux kernel resource isolation features (such as cgroups and 
kernel namespaces) and support for federated file systems (such as
 OverlayFS, etc.) to allow independent containers to run in a 
single Linux instance, thus avoiding startup. And keep the overhead
 of the virtual machine (VM). The Linux kernel's support for 
namespaces often isolates the application's view of the operating 
environment, including the process tree, network, user IDs, and 
mounted file systems, while the kernel's cgroups provide memory and
 CPU resource limits. Beginning with version 0.9, Docker uses the 
libcontainer library as its own way to directly use the 
virtualization tools provided by the Linux kernel, in addition to 
using libvirt, LXC, and systemd-nspawn to use the abstract 
virtualization interface\cite{editor10}.

\section{Application of Data Mining in Psychological Research}

Data mining methods can be roughly divided into two categories: 
supervised learning methods and unsupervised learning methods. In 
supervised learning, there is an interesting result. The goal is to
 develop a predictive model based on a set of variables. Most 
supervised learning methods focus on variable selection, 
nonlinearity, and interactive effects, and therefore have many 
advantages over standard regression models. Regression models with
 a large number of variables may be unstable, especially if there 
is a high degree of correlation between predictors. In addition, 
when the number of variables is large, it may be almost impossible
 to manually search for which interactions may exist. The goal of 
supervised learning methods is to identify variables that are 
important variables, nonlinear forms and/or their interaction 
effects. These methods usually produce a simpler and more 
interpretable model because important influences can be isolated. 
In addition, the final model is more likely to be duplicated in the
 new sample\cite{editor11}.

In unsupervised learning, we do not want to explain any outcome 
variables; instead, our goal is to group variables or participants
 based on their similarities or covariances. Unlike supervised 
learning methods, unsupervised learning is often used in 
psychological research. For example, data reduction methods such as
 principal component analysis (PCA) and exploratory factor analysis
 (EFA) are very common in psychology, and methods for grouping 
participants are also common, such as cluster analysis and finite 
mixture models\cite{editor11}.

Supervised learning methods are rarely used in psychology; however,
 these methods should and will play a greater role in future 
psychological research. As we pointed out, one reason that these 
methods may not be adopted in psychology is because researchers may
 think that these methods require large amounts of data - a large 
number of participants and a large number of variables. It is worth
 noting that many data mining methods work well in small data 
setups. For example, when considering losses due to losses, 
classification and regression trees exceeded multiple inferences in 
small samples. As a second example, it was found that the use of 
shrinkage in the modeling of Bayesian 
structural equations produces less biased samples than the maximum
 likelihood estimation\cite{editor11}.

As we have pointed out, unsupervised learning methods are very 
common in psychology. PCA and EFA are common data reduction 
methods, and EFA is often the first step in understanding the data
 dimension. In many cases, the EFA model is applied to one-half of
 the data set, and then a confirmatory factor analysis (CFA) model
 is estimated based on the remaining half of the data to separate 
the exploration from the validation of the data analysis. This 
method is similar to cross-validation, but researchers in 
psychology often do not validate the exact model. In general, the 
model is re-estimated in the CFA, and the negligible factor load 
in the EFA is fixed at zero in the CFA\cite{editor11}.

Similar to PCA and EFA, cluster analysis and finite mixture models
 are common in psychology and social sciences. Limited-mix models 
are increasingly used to search for groups with different data 
patterns or associations. In psychology, few influences are 
universal. The finite mixture model is a way for researchers to 
search for conditional effects. One problem with using the finite 
mixture model in psychology is that cross-validation is rarely used
 to assess the feasibility of the model. However, cross-validation
 has recently received more attention in hybrid modeling\cite{editor11}.

Although supervised learning methods are not commonly used in 
psychology, most of them can be attributed to the lack of attention
 paid by these methods from scholars of psychological science 
methods. As more and more data mining methods gradually apply to 
the nuances and complexity of psychological data and methods\cite{editor11}, 
this situation is 
slowly and surely changing. Specifically, many of these big data 
approaches with latent variable models are focused common in psychology.

Due to our multivariate measurement and our fairly common vertical
 design, latent variable models (eg, confirmatory factor models, 
structural equation models [SEM]) are common in psychology. 
Combining data mining algorithms with latent variable models is a 
necessary step to increase the use of psychologists. There have 
been several recent examples of such integration. For example, 
Brandmaier, von Oertzen, McArdle and Lindenberger\cite{editor11} developed 
SEM trees by combining SEM with classification and regression 
tree algorithms. In the SEM tree, the data is divided using a 
series of predictors, and the user-specified SEM fits into each 
partition of the data. The goal is to find the predictors that 
maximize the fit of the model. Essentially, this is a method of 
automatically searching for a group of participants, where members
 of the same group are homogeneous about SEM, and members of 
different groups are heterogeneous about SEM\cite{editor11}. 
For example, a SEM tree can be used to
 find groups with different time trajectories or groups with 
different measurement models.

Similarly, Jacobucci, Grimm and McArdle (2016) combined 
regularization methods commonly used in high-dimensional regression
 with SEMs to create regularized SEMs (RegSEM). RegSEM allows 
researchers to punish specific parameters in the SEM, resulting in
 a simpler, more replicable SEM. There are similar developments in
 the multi-level modeling framework. For example, Hajjem, 
Bellavance and Larocque (2011) and Sela and Simonoff (2012) combined
 a mixed effects model with a regression tree to create a 
mixed-effect regression tree. These methods can efficiently search
 high-dimensional hierarchically structured data for nonlinear and
 interactive effects\cite{editor11}.

Although recent work has made some algorithms more applicable to 
social scientists, we emphasize the difficulty of gaining 
attention - incomplete data. In summary, many data mining 
algorithms require complete data. Moreover, different programs 
handle incomplete data in different ways. Since incomplete data are
 common in psychological research and are usually not lost 
completely at random, the model may produce biased results, or at 
least the result depends on the data used to deal with 
incompleteness. Therefore, an approach to future research will 
greatly increase the usefulness of these methods in psychological 
research, incorporating contemporary missing data methods (such as
 multiple attribution or comprehensive information estimates) into
 data mining plans.

\section{Application of Data Mining to Predict Real Life Outcomes}

This section is based on the study of Mining Big Data to Extract 
Patterns and Predict Real-Life Outcomes\cite{editor12}.

Human activities are increasingly affected by digital products and
 services. Individuals use social networking sites to communicate 
with messaging applications, use online platforms and credit cards
 for payment, and transmit digital media. In addition, they are 
virtually inseparable from wearable devices such as fitness 
trackers and smart phones. Increasingly immersed in the digital 
environment and relying on digital devices means that people's 
behavior, communication, geographic location and even physical 
status can be easily recorded, resulting in a large number of 
digital footprint samples. These footprints include web browsing 
logs, transaction records from online and offline markets, photos 
and videos, GPS location logs, media playlists, voice and video 
call records, languages used in tweets or emails, and more.

Recent research demonstrates that digital footprints can be 
successfully employed to study important psychological outcomes 
ranging from personality, language, and emotions to cultural fit
 and social networking. Unfortunately, an interest in studying 
digital footprints, as well as the necessary skills to do so, are 
relatively rare among social scientists. Therefore, this kind of 
research is increasingly assigned to computer scientists and 
engineers. They often lack theoretical background in social 
sciences and ethical standards training on human subject research.

The following of this section will describe the steps of applying 
big data technique in psychology, most of the codes are written by R.

Data Sets of Digital Footprints

This section focuses on how to import, store and preprocess large 
samples of digital footprints. Many large data sets can be obtained
 online for free, or can be obtained from companies and 
institutions that collect and store them. In a popular example, 
myPersonality.org database 1 stores scores from dozens of 
psychological questionnaires and Facebook profile data for more 
than six million participants. The Stanford University Web 
Analytics Project website2 has a wide range of datasets, including
 social networking, Tweets, and product reviews. In addition, 
journal articles are now often accompanied by publicly available 
data sets. A recent article by Eichstaedt et al. (2015), 
supplemented by data sets aggregated at the county level by U.S. 
users.3 In addition, many online platforms (such as Twitter) 
contain large amounts of publicly available data that can be easily 
recorded. Peer website provides a collection links to other 
potentially interesting data sets. The following hands-on section 
describes the sample data set used in this tutorial. The sample 
data set was obtained from the myPersonality.org database.

The data set used here contains psychodemographic profiles of 
110,728 Facebook users and their Facebook Likes.For simplicity and
 ease of management, the sample is limited to U.S. users. The 
following three files can be downloaded from the companion website:

1.\ user.csv: Contains psychodemographic user profiles. It has 
110,728 lines (not including line and column names) and 9 columns:
 anonymous user ID, gender (male for 0, female for 1), age, 
political opinion (Democrats for 0, and 1 for Republicans) 
as well as five scores in the 100-item International Personality 
Program Questionnaire to assess five factors (ie, openness, 
discretion, extroversion, agreeability, and neuroticism) 
personality models.

2.\ likes.csv: Contains anonymous ID and 1,580,284 Facebook like 
names. It has two columns: ID and name.

3.\ users-likes.csv: Include associations between users and their 
likes, stored as user class pairs. It has 10,612,326 rows and two 
columns: User ID and Like ID. The existence of a similar user pair
 means that a given user has a corresponding Like on their profile.

To load the data files into R:

\begin{verbatim}
users <- read.csv('users.csv')
likes <- read.csv('likes.csv')
ul <- read.csv('users-likes.csv')
\end{verbatim}

Constructing a User–Footprint Matrix

\begin{verbatim}
ul$user_row <- match(ul$userid,users$userid)
ul$like_row <- match(ul$likeid,likes$likeid)
\end{verbatim}

Use the pointers to rows in the users and likes objects to build 
a user–Like matrix using the sparseMatrix function from the Matrix
 library:

\begin{verbatim}
require(Matrix)
M <- sparseMatrix(i=ul$user_row, j=ul$like_row, x=1)
\end{verbatim}

The parameters i and j of the sparseMatrix function indicate the 
positions of the non-zero cells of the matrix (rows and columns, 
respectively). The parameter x represents the value of each cell. 
As mentioned earlier, Facebook users can only issue Like once, so 
we set all non-zero units to x equal to 1.

Finally, the row-name of the similar user's Matrix M is set to 
contain the ID of each user, and the column name of M is set to 
contain each favorite name.

\begin{verbatim}
rownames(M) <- users$userid
colnames(M) <- likes$name
dim(M)
\end{verbatim}

Trimming the User–Footprint Matrix

There is no single or simple, correct way to select the lowest 
frequency, below which frequency a given user or step should be 
deleted, but the following rules can be used to make the decision.
 First, remove individual user and footstep instances from the 
data because they are not useful for the extraction mode. Second, 
consider the available hardware and computing time. Keeping too 
many data points may increase the time and memory required. On the
 other hand, deleting too many data points can significantly reduce
 the amount of information available for analysis. Therefore, it is
 recommended to plan and analyze a small number of randomly 
selected small samples of different sizes in order to approximate 
the relationship between the required data size and time and 
memory. This not only tells you how much data you need to keep, 
but also speeds up writing code by reducing the time required for 
testing.

Hands-On: Trimming the User–Like Matrix

\begin{verbatim}
repeat {
i <- sum(dim(M))
M <- M[rowSums(M) >= 50, colSums(M) >= 150]
if (sum(dim(M)) == i) break
}
\end{verbatim}

This code uses repeated loops until interrupted by a command 
interrupt. Inside the loop, we first set i to contain the sum of 
the dimensions of M (that is, the total number of rows and 
columns). Next, we only keep the rows and columns that contain at 
least the elements of the preset thresholds 50 and 150. Finally, 
we check if the size of M has changed. If so, the loop will break;
 otherwise, it will continue.
Next, users deleted from M are removed from the users object:

\begin{verbatim}
users <- users[match(rownames(M),
users$userid),]
\end{verbatim}

Extracting Patterns from Big Data Sets

Reducing the dimensionality of the data (or extracting the cluster)
 has many advantages. First, in the case of large data sets, there
 are usually more variables than users. In this case, reducing the
 dimension is very important because most statistical analysis 
requires more users than variables (preferably more). Second, even
 if the number of users exceeds the number of variables, further 
reductions can reduce the risk of overfitting and may increase the
 statistical power of the results. Third, reducing dimensions by 
eliminating multiple collinearity and redundancy in the data group
 related variables into a dimension or cluster. Fourth, a small 
number of dimensions or clusters containing data are easier to 
interpret than hundreds or thousands of individual variables. 
Finally, reducing the dimensions reduces the computational time and
 memory required for further analysis.

Selecting the Number of Dimensions or Clusters to Extract

One of the main considerations for reducing the data dimension is 
to select the correct number of dimensions or clusters (in k) to be
 extracted. Unfortunately, there is no single (or simple) correct 
way. Moreover, the expected value of k depends on the intended 
application. If the goal is to gain insight from the data, several
 dimensions or clusters may be easier to interpret and visualize. 
On the other hand, if the goal is to build a predictive model, more
 dimensions or clusters will retain more information from the 
original matrix to achieve more accurate predictions. However, k is
 set too high, and the benefits of dimension reduction discussed 
earlier are lost, and the prediction accuracy may be reduced. The 
following sections discuss SVD and LDA in more detail and introduce
 some simple ways to choose the correct value for k.

Singular Value Decomposition

VD denotes a given matrix of the products of three matrices (m rows
 and n columns): The matrix U (with size mk) containing the left 
singular vector contains a non-negative square diagonal matrix 
(size k); the matrix V (size nk) contains The correct singular 
vector, where k is the dimension that the researcher chooses to 
extract. For simplicity, we refer to the left and right singular 
values as SVD dimensions.

Centering the data
Before performing SVD to improve the interpretability of the SVD 
dimension, the data is usually centered (ie, the entries in the 
matrix are reduced column by column). This is because the first SVD
 dimension extracted from non-central data is closely related to 
the frequency of objects in rows and columns.11 Since the remaining
 dimensions must be orthogonal to the first dimension dimension, 
the resulting SVD dimension may not be well represented data.

Rotation
The SVD aims to maximize the variance considered by the first and 
subsequent orthogonal dimensions. Therefore, early SVD dimensions 
are highly relevant to many users and footprints. In addition, many
 users and footprints are related to many SVD dimensions, making 
SVD results difficult to interpret.

Computing SVD
Most programming languages provide ready-made packages for 
calculating SVD: the svd function of Python's SciPy library, the 
feature library in C, the PROPACK library in Matlab, and the 
irlba package in R. For sparse or very large matrices, it is 
recommended to use SVD sparse variants available in most of the 
above packages. You can use the various functions available in R 
(for example, varimax or promax), Matlab, and other languages for 
factor rotation.

Latent Dirichlet Allocation
LDA assumes that each user and each footstep in the matrix (in the
 case of LDA, the user is referred to as a document, and the 
footprint as a word) belongs to a group of k clusters (known as 
topics) with a certain probability. For matrices of size m rows and
 n columns, the LDA generates a matrix of size mk, describing the 
probability that each user belongs to each cluster, and a matrix of
 size kn that describes the probability that each cluster belongs 
to each cluster. 

Selecting the k
As with other methods, there is no single or simple way to select 
the correct number k of LDA clusters to extract. Common methods use
 the log-likelihood estimate of the model and can plot the number k
 of clusters extracted. This requires the generation of several 
models for different k values. Generally, for a lower range of k, 
the log-likelihood grows rapidly, flattens at higher k-values, and
 may begin to decrease once the number of clusters becomes very 
large. Choosing a marker that quickly increases the log-likelihood
 at the end of k usually explains these topics well. Larger 
k-values generally provide better predictive power. Alternatively,
 the interpretability of the topic may be supported by applying a 
hierarchical LDA.

Computing LDA
Most programming languages provide ready-made LDA implementations: Rs 
theme model package, Pythons scikit-learn package, 
GibbsLDA++ for C++, and theme modeling toolbox for Matlab.

Hands-On: Reducing the Dimensionality of the
User–Like Matrix Using SVD and LDA

\begin{verbatim}
set.seed(seed = 68)
library(irlba)
Msvd <- irlba(M, nv = 5)
u <- Msvd$u
v <- Msvd$v
\end{verbatim}

The following code produces Vrot and Urot, the varimaxrotated
equivalents of matrices U and V

\begin{verbatim}
v_rot <- unclass(varimax(Msvd$v)$loadings)
u_rot <- as.matrix(M %*% v_rot)
\end{verbatim}

As in the case of the SVD analysis, seed 68 was used as the preset
 R’s random number generator to ascertain that the results 
presented here and computed by the readers are the same:

\begin{verbatim}
library(topicmodels)
Mlda <- LDA(M, k = 5, control = list(alpha =
10, delta = .1, seed = 68), method =
'Gibbs')
gamma <- Mlda@gamma
beta <- exp(Mlda@beta)
\end{verbatim}

Consider expanding the test scope of ks, but note that this code 
may take a long time to run:

\begin{verbatim}
lg <- list()
for (i in 2:5) {
Mlda <- LDA(M, k = i, control =
list(alpha = 10, delta = .1, seed = 68),
method = 'Gibbs')
lg[[i]] <- logLik(Mlda)
}
plot(2:5, unlist(lg))
\end{verbatim}

Interpreting Dimensions and Clusters

Several strategies can be used to explain the nature of patterns 
extracted from large data sets. First, you can explore the 
footprints that are most relevant to a given dimension or cluster.
 This method is used to explain the dimension and clustering 
extracted from the user movie Matrix X in the extracting schema 
part from the big data set. Second, you can examine the 
relationship between dimensions and clusters as well as some of 
the user's known properties (for example, demographic information,
 psychological characteristics, responses to individual questions 
on the psychological questionnaire, etc.). Finally, you can 
explore the interpretation of dimensions or clusters by exploring 
them a hierarchy that spans different k levels. The equivalent of 
LDA is provided by graded LDA.

Hands-On: Interpreting Clusters and Dimensions

\begin{verbatim}
cor(gamma, users[,-1], use = 'pairwise')
\end{verbatim}

LDA1 members were negatively correlated with age, positively 
correlated with gender (male 0, female 1), political opinion 
(democrats 0, Republicans 1), indicating that this group was mainly
 reserved for young people. Female. Cluster LDA2 is similar but has
 nothing to do with gender. The group LDA3 contains elderly, 
liberal and politically free men. Compared with other groups, the 
correlation of LDA4 with the psychological population 
characteristics included in this sample was low. The last cluster 
LDA5 has the closest relationship with the personality dimension. 
Emotionally stable, happy, outgoing, responsible, conservative 
(including personality and political views), older and female users.

The strength of the relationship between likes and clusters is 
stored in the R object beta. The following code can be used to 
extract the top 10 Likes that are most closely associated with each
 cluster:

\begin{verbatim}
top <- list()
for (i in 1:5) {
f <- order(beta[i,])
temp <- tail(f, n = 10)
top[[i]] <- colnames(M)[temp]
}
top
\end{verbatim}

SVD dimensions

A similar approach can be used in the SVD dimension. Use the 
following code to obtain the correlation between the maximum 
likelihood rotated SVD dimension (Urot) and the user score for the
 mental demographic user function:

\begin{verbatim}
cor(u_rot, users[,-1], use = 'pairwise')
\end{verbatim}

Just as in the case of LDA clusters, one can also study the 
association between the like and the maximum-maximum rotation SVD 
dimension. For the benefit of space, we do not include these 
results here, but encourage readers to make them. Please note that
 Vrot is an equivalent in LDA. Moreover, since SVD produces a 
negative correlation and a positive correlation, it is convenient 
to separately consider the most positive and negatively related 
footprints for a given SVD dimension.

The loop previously used to extract the loop of the most 
representative LDA cluster can be used to extract the likes of the
 10 highest scores and 10 lowest scores in the SVD dimension. To 
rank likes based on their score on the ith SVD dimension, use the 
following command:

\begin{verbatim}
f <- order(v_rot[,i])
\end{verbatim}

To extract a favorite index with extreme scores, use the tail and 
head functions to provide the n last or first elements of the 
object:

\begin{verbatim}
colnames(M)[tail(f, n = 10)]
colnames(M)[head(f, n = 10)]
\end{verbatim}

Predicting Real-Life Outcomes

This part focuses on building a prediction model based on 
dimensions and clusters extracted from the user's footprint matrix.
 14 A large number of methods can be used to build predictive 
models based on large datasets, from relatively complex methods 
(such as deep learning, neural networks, probability map models, or
 support vector machines) to simpler methods (such as linear and 
logistic regression). In practice, it is wise to start with a 
simple forecasting method. They are faster and easier to implement,
 and they provide a good benchmark for judging quality and 
debugging more complex methods. In addition, according to our 
experience, simple models (such as linear and logistic regression)
 often provide similar accuracy for more complex methods - and are
 easier to interpret. Finally, using simple methods can reduce the
 risk of errors, maximize the transparency of the method, and 
promote the reproducibility of results. In this work, we use linear
 and logistic regression, which is well known to most social 
scientists.

Cross-Validation

To reduce the variability of results, multiple rounds of 
cross-validation are often performed using different data 
partitions. For example, k-fold cross-validation divides the data 
into k (usually k = 10) subsets of the same size (called folds). 
The model was built on a training subset consisting of all 
one-to-one (k-1) folds and verified on the excluded subset of 
tests. This process is repeated k times for each subset and the 
accuracy is averaged over all trials.

Cross-validation can be easily done in R and other statistical 
languages, such as using for loops, but there are also 
well-specified libraries. Popular R libraries that support 
cross-validation include boot and caret. For Python users, we 
recommend using the cross-validation module in scikit-learn.

Hands-On: Predicting Real-Life Outcomes With
Facebook Likes

The following code generates a vector collapse with a length equal
 to the number of users, consisting of randomly selected numbers 
ranging from 1 to 10:

\begin{verbatim}
folds <- sample(1:10, size = nrow(users),
replace = T)
\end{verbatim}

Next, users with a folding value equal to 1 are assigned to the 
test subset and the remaining users are assigned to the training 
subset. The following code generates the logic vector15 test, if 
the object is collapsed to 1, the test result is TRUE, otherwise 
returns FALSE:

\begin{verbatim}
test <- folds == 1
\end{verbatim}

Logical vectors can be used to extract the desired elements from R
 objects, such as other vectors or matrices. For example, you can 
use the command mean (users age [test]) to calculate the average age
 of users in the test subset. In addition, the true/false elements
 of the logic vector can be easily represented using logical 
operators rather than [!], so instead of creating a separate 
logical vector vector indicating the degree of membership in the 
training subset, you can simply use the test! test. In the 
following code, test and test vectors are used to access test and 
training subsets, respectively.

Therefore, the SVD scores of the users in the test subset are based
 on the analysis performed only on the training subset, which 
preserves the independence of the results obtained from the 
training and test subsets:

\begin{verbatim}
Msvd <- irlba(M[!test,], nv = 50)
v_rot <- unclass(varimax(Msvd$v)$loadings)
u_rot <- as.data.frame(as.matrix(M %*%
v_rot))
\end{verbatim}

Finally, it is pointed out that all the independent variables 
should be used to predict the variable ope in the object user.

\begin{verbatim}
fit_o <- glm(users$ope, data = u_rot,
subset = !test)
fit_g <- glm(users$gender, data = u_rot,
subset = !test, family = 'binomial')
\end{verbatim}

Generate predictions using the models (fito and fitg) developed in
 the previous step and the user's maximum rotation SVD score 
(urot [test,]) in the test subset:

\begin{verbatim}
pred_o <- predict(fit_o, u_rot[test,])
pred_g <- predict(fit_g, u_rot[test,], type =
'response')
\end{verbatim}

The accuracy of linear prediction (eg, openness) can be easily 
expressed as a Pearson product moment correlation:

\begin{verbatim}
cor(users$ope[test], pred_o)
\end{verbatim}

So far, we estimate the prediction performance based on all the SVD
 dimensions of k = 50. Here, we will study the relationship between
 cross-validation prediction accuracy and the SVD dimension k. In 
the following code snippet, we first define a set ks containing the
 k values we want to test. It includes all values from 2 to 10, 15,
 20, 30, 40 and 50. Next, we create an empty list rs to store the 
results. Then we start a for loop, rerun the code surrounded by 
braces, and change the value of i to a continuation element stored
 in ks. The code in parentheses is similar to the code used 
previously, but only the first of the k = 50 SVD dimensions (Msvd 
 v [,1:i]) is used. The result is stored as a new element in the 
list rs (rs [[as.character (k)]]). We fix R's random number generator
 again to ensure that our results match the reader's results:

\begin{verbatim}
set.seed(seed = 68)
ks<-c(2:10,15,20,30,40,50)
rs <- list()
for (i in ks){
v_rot <- unclass(varimax(Msvd$v[,
1:i])$loadings)
u_rot <- as.data.frame(as.matrix(M%*%
v_rot))
fit_o <- glm(users$ope, data = u_rot,
subset = !test)
pred_o <- predict(fit_o, u_rot[test,])
rs[[as.character(i)]] <- cor(users$ope
[test], pred_o)
}
\end{verbatim}

Finally, we calculate the open cross-validation forecast for the 
entire sample. To do this, we build a model and use the data 10 
times to calculate the predicted value. First, we create a vector 
predo to store the prediction of the entire sample. It is full of 
missing values (NA) and has as many elements as the number of 
users. Next, we use the for loop to re-run the previously described
 code while selecting a continuous cross-validation fold. Each 
time, the predicted openness value is saved as an element of the 
predo vector. After the loop is completed, we calculate the 
correlation between the actual open score and the predicted open 
score for all users in the sample:

\begin{verbatim}
set.seed(seed = 68)
pred_o <- rep(NA, n = nrow(users))
for (i in 1:10){
test <- folds == i
Msvd <- irlba(M[!test,], nv = 50)
v_rot <- unclass(varimax(Msvd$v)$loadings)
u_rot <- as.data.frame(as.matrix(M %*%
v_rot))
fit_o <- glm(users$ope, data = u_rot,
subset = !test)
pred_o[test] <- predict(fit_o, u_rot[test,])
}
cor(users$ope, pred_o)
\end{verbatim}

The prediction accuracy for openness, estimated on the entire
sample, is r = 0.44.

Try wrapping it within a for loop, similar to the one used for 
producing SVD-based predictions:

\begin{verbatim}
Mlda <- LDA(M[!test,], control =
list(alpha = 1, delta = .1, seed = 68),
k = 50, method = 'Gibbs')
temp<-posterior(Mlda, M)
gamma <- as.data.frame(temp$topics)
\end{verbatim}

\section{Conclusion and Discussion}

Therefore, Big data is very likely to provide value to psychology.
 However, the pursuit of big data is still an uncertain and risky 
career for ordinary psychology researchers.

How can psychology adapt to the field of big data or computing 
social sciences and other related fields? Psychology has begun to 
cover many areas such as health, mental health, depression, 
substance use, behavioral health, behavioral change, social media,
 workplace well-being and effectiveness, student learning and 
adjustment, and behavioral genetics. Researchers are working on 
topics such as health and human conditions in large data sets that
 contain thousands of people. Researchers envision research that 
can link these personal data with health and productivity, 
revealing patterns or connections between behavior and various 
outcomes of interest.

Obviously, big data or data science stay here, with or without 
psychology. This broad and growing field provides interested 
thinkers with a unique opportunity to address the complex 
technical, substantive, and ethical challenges associated with 
storing, retrieving, analyzing, and validating large data sets. 
Big data science can jointly discover and elucidate the persuasive
 and powerful patterns in psychological data that directly or 
indirectly involve human behavior, cognition, and the effects of 
time and socio-cultural systems. In turn, these mental models give
 non-psychological data (such as medical data related to 
health-related interventions; prosperity and depression related to
 financial investment behavior). The big data community and big 
data itself can jointly promote the development of psychological 
science.


\begin{acks}

  The authors would like to thank Dr.~Gregor~von~Laszewski for his
  support and suggestions to write this paper.

\end{acks}


\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

