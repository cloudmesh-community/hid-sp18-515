% status: 0
% chapter: TBD

\title{Big Data in Pychology}


\author{Qingyun Lin}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{Smith Reserch Center}
  \city{Bloomington} 
  \state{IN} 
  \postcode{47408}
}
\email{ql10@iu.edu}


% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{G. v. Laszewski}


\begin{abstract}
There is much discussion about the value of company big data. 
For example, Amazon will match your shopping and page views with 
other shoppers’ purchases and pageviews, and try to find people with 
similar interests. Then Amazon recommends buying products that people 
like, assuming you will like them.

Can big data be applied to answer the research community’s interest 
in psychology?

This paper will introduce what is pychology and what is big data 
technique simply, and why these two fields can be combined togethor, 
how can the society benefit from this combination, and introduce 
several combination examples and what is the future of this field.
\end{abstract}

\keywords{hid-sp18-515, big data, pychology, combination, future}


\maketitle

\section{Introduction}

Pychology problem is getting more and more attention in the morden 
society. And as the big data technology is such a heated topic today, 
the combination of this two fields should be well applied.

Big data is particularly good at dealing with issues that people may 
be reluctant to answer surveys. Often, the way people interact with 
computers reveals the interest they would not express in interviews 
or anonymous surveys\cite{editor00}.

Significant advances in computing technology, coupled with the 
proliferation of large digital networks and social media platforms, 
have produced an unimaginable amount of information about people. 
It is estimated that the amount of digital data currently present is 
in the thousands of exabytes or 10 to the 18th power of bytes.

This era of big data is likely to change the way psychologists observe 
human behavior. However, just as it creates new opportunities, access 
to a large amount of information also brings new challenges to research. 
Michael N. Jones of Indiana University Bloomington introduced the 
Big Data Project at the 2014 APS Annual Conference.

Each little piece of data is a trace of human behavior and offers 
us a potential clue to understanding basic psychological principles, 
said Jones. But we have to be able to put all those pieces together 
correctly to better understand the basic psychological principles 
that generated them\cite{editor01}.

The study of language development as one of Jones's own research interests 
is a good example of big data research. Collecting a large number of infant
samples from the natural environment is time-consuming and often results 
in small sample sizes. The test theory of the way children learn 
languages is a long time.

Big data can help speed up the process. As a proof of concept, Jones 
showed that based on associative learning theory, more than 100,000 
words from natural language can be entered into a computer model - the 
idea is that children group words together according to how often 
they use them near other words. Jones said that as the analysis progressed, 
the model did realize that the relationship between computers and data 
and word classes was closer than computers and obsolete.

Jones said that in the end, a similar analysis can be used to study the 
theory of connected learning in direct child conversation samples. As 
long as they have enough data to continue, these models are very good 
at learning from noise, he said\cite{editor01}.

Tanzeem Choudhury, an information scientist at Cornell University, 
said that big data may help researchers reach the point where they 
can collect behavioral information without sampling human participants. 
Technologies such as smart phones and wearable sensors can collect 
information about physical activity, social interaction, geographic 
location, and more.

The result of this data collection is that it is effective for users; 
it does not require their time or effort, and it greatly reduces 
self-reporting errors.

We can continuously get measurements of behavior without bugging 
people to fill out surveys, said Choudhury. We can potentially get 
continuous measurement without actually having to engage users all 
the time and rely on their self-input\cite{editor01}.

Chowdhury participated in some of these projects. StressSense can track 
daytime stress and help them avoid anxiety. For example, MyBehavior uses 
physical activity patterns to suggest ways of maintaining the shape, for 
example, working more frequently along lines that the user seems to like. 
MoodRhythm allows patients with bipolar disorder to monitor sleep and social 
interactions to maintain emotional and energy balance, which is a major 
improvement in pen and daily behavioral tracking. (These programs are 
still under development because of smartphone apps.)

Big data has produced positive changes in the online search field. 
Susan T.Dumais of Microsoft Research Inc.\ said that the billions of Internet 
searches that happen each day leave the behavior log that analysts use 
to improve search engines. Without such a large record, sites such as 
Google and Bing will never be able to use 2.4 words in an average Internet 
search and turn it into useful content.

Behavioral logs allow us to characterize, with a richness and fidelity 
that we’ve never had before, what it is people are trying to do with
the tools and systems they’re interacting with, said Dumais\cite{editor01}.

By mining behavior logs, analysts can create personalized algorithms 
to improve the user's search experience. For example, if Dumais searches 
for sigir, she may need the homepage of the Information Search Special 
Interest Group (SIGIR). If Little Stuart Bowen performs the same search, 
he may need the site to hold his position: Special Inspector General 
for Reconstruction of Iraq (also known as SIGIR).

In other words, the system can know that words and acronyms do not always 
predict the best way users want to search. Modeling the search in a way 
that considers the context of the published query is very important for 
improving Web search. Previous search activity is important, and the 
location and time of the query are also important. For example, the 
search for Spring US Open may refer to a golf ball, while the same 
search in late summer may refer to tennis.

Before you were able to collect Big Data, the person who spoke loudest, 
or the highest-paid person’s opinion, would dominate, said Dumais. Now 
the data, especially when derived from carefully controlled Web-scale 
experiments, dominates.\cite{editor01}

Big Data also allows researchers to rethink past issues in new ways, 
said Brian M. D'Onofrio, an APS researcher at Indiana University Bloomington. 
In particular, he pointed out that researchers should consider re-adjusting 
data that may be collected for other reasons. Reusing big data samples can 
help researchers generate insights. Traditional samples can't achieve the 
statistical power that many labs lack. This is a big challenge for psychology 
to improve its methods and replication process.

With Big Data, it gives you the opportunity to use several different 
types of quasi-experimental designs, to help rule out alternative 
explanations, D’Onofrio said\cite{editor01}.

D'Onofrio and his collaborators recently rearranged the millions of 
personal records compiled by Sweden in order to challenge the traditional 
notion that smoking during pregnancy directly leads to subsequent negative 
behavioral outcomes such as crime. In one study, the researchers analyzed 
50,000 siblings whose mothers smoked but did not smoke during pregnancy. 
They determined the family context factor, not smoking during pregnancy, 
related to criminal convictions. This recognition can greatly improve 
interventions: 
In this case, allowing women to quit smoking should only be part of 
broader social services.

Tal Yarkoni of the University of Texas at Austin says big data can 
even help psychologists study research. Yarkoni and others recently 
developed Neurosynth, an online program for analyzing a large number 
of fMRI data to guide users to topics of interest. Yarkoni said that 
so far, Neurosynth has synthesized studies from more than 9,000 
neuroimaging studies and about 300,000 brain activations.

One of Neurosynth's main goals is to distinguish brain activity that 
is always associated with a specific psychological process but not 
specific, and high-probability brain activity that means having a 
specific mental process. For example, painful physical stimulation 
may continue to produce patterns of brain activity, but this pattern 
of activity does not necessarily mean the presence of pain; other 
mental states may produce similar patterns. Infer mental processes 
from observed brain activity, this process is called reverse 
reasoning, it is difficult to do in a single neuroimaging study.

Neurosynth overturned reasoning by accumulating a large number of 
images and research data in one place. For example, even if some 
active brain regions overlap in these three cases, the database can 
help researchers find areas of the brain that are particularly relevant 
to pain, not working memory or emotions. Tests have shown that, in many 
cases, Neurosynth's performance and analysis are done manually by 
screening research literature - saving hundreds of hours of research 
time compared to just pressing a button, Yarkoni said.

That’s the long-term goal, Yarkoni said. To do this in a quantitative, 
automated way instead of a manual, qualitative way\cite{editor01}.

The long term goal is the theme of the plan, because big data is 
still the existence of science. Not everyone believes that it will 
bring a paradigm shift. Even if big data does change statistical analysis, 
it cannot replace Strong behavioral theory or experiment. But in terms of 
improving these theories or strengthening big data for these experiments, 
researchers cannot ignore it.

\section{Simple Introduction of Psychology}

Psychology is the science of behavior and mind, including conscious 
and unconscious phenomena, as well as feelings and thoughts. This is 
a wide range of subjects with diverse interests. When they are 
together, they try to understand the sudden nature of the brain and 
all the various post-show phenomena they exhibit. As a social science,
 it aims to understand individuals and groups by establishing general
 principles and studying specific cases.

In this area, professional practitioners or researchers are called 
psychologists and can be classified as social, behavioral, or 
cognitive scientists. Psychologists try to understand the role of 
mental function in personal and social behavior, and also explore 
the physiological and physiological processes behind cognitive 
function and behavior.

Psychologists explore behavioral and psychological processes, 
including perception, cognition, attention, emotion (emotion), 
intelligence, phenomenology, con lation, brain function and 
personality. This extends to human interactions such as interpersonal
 relationships, including psychological resilience, family resilience,
 and other areas. Psychologists in different directions also consider
 unconscious thoughts. Psychologists use empirical methods to infer 
causality and correlations between psychosocial variables. In 
addition, or on the contrary, using experience and deduction methods,
 some clinical and counseling psychologists sometimes rely on 
symbolic explanations and other inductive techniques. Psychology is 
described as a central science whose psychological research results 
are linked to research and social sciences, natural sciences, 
medicine, humanities, and philosophy.

Although psychology knowledge is often used to assess and treat 
mental health problems, it also points to understanding and solving 
problems in several areas of human activity. Many people's psychology
 is ultimately for the benefit of society. Most psychologists are 
involved in clinical, counseling or some kind of treatment and 
practice in the school environment. Many people usually work in 
university psychology or other academic environments (such as medical
 schools, hospitals) and engage in extensive scientific research on 
psychological processes and behaviors. Some people are employed in 
industrial and organizational environments or other fields such as 
human development and aging, sports, health and media, as well as 
legal investigations and other aspects of the law.

\section{Big data}

Big data is a data set. It has a large number and complexity. 
Traditional data processing applications are not enough to cope with 
this data. Big data challenges include capturing data, data storage, 
data analysis, search, sharing, transmission, visualization, querying,
 updating, information privacy, and data sources. There are a lot of 
concepts related to Big Data: There are 3 conceptual volumes, breeds,
 and speeds. Other concepts that were later triggered by big data are
 accuracy (ie, how much noise in the data) and value.

Recently, the term big data tends to refer to the use of predictive 
analytics, user behavior analysis, or some other advanced data 
analytics methods that extract value from data and are rarely used for
 data sets of a specific size. There is no doubt that the amount of 
data available today is indeed large, but this is not the most 
relevant feature of this new data ecosystem. Dataset analysis can 
find new relevance to discover commercial trends and prevent 
diseases, Combat crime, etc..  Scientists, corporate executives, 
medicine, advertising and government practitioners often encounter 
difficulties in large-scale data collections including Internet 
search, financial technology, city information and business 
information. Scientists have encountered limitations in the work of 
electronic sciences, including meteorology, genomics, linkomics, 
complex physics simulations, biology and environmental studies.

Datasets are growing rapidly - partly because they are increasingly 
using portable and mass information-aware IoT devices (such as mobile
 devices, antennas (remote sensing), software logs, cameras, 
microphones, radio frequency identification (RFID) readers) Since 
the 1980s, the amount of information stored by the world's technology
 per capita has roughly doubled every 40 months; As of 2012, it 
produced 2.5 exabytes per day. Data According to IDC, the global 
data volume will increase from 4.4 zettabytes in 2013 to 2020 to 44 
zeta. By 2025, IDC predicts 163 megabytes of data. For large 
companies, one issue is to determine who should have big data 
initiatives that affect the entire organization.

Relational database management systems and desktop statistics, 
visualization of data packets is often difficult to handle big data. 
This work may require a lot of parallel software running on dozens, 
hundreds, even thousands of servers. What is big data depends on the 
functionality of users and their tools, and the expansion of big data
 as a moving target. For some organizations, facing hundreds of 
gigabytes of data for the first time may require reconsidering data 
management options, and for other organizations, it may take tens or 
hundreds of terabytes before data size becomes an important 
consideration.

Data Mining Algorithms

Data mining is an iterative process intended to discover patterns
in large quantities of data. These patterns are then used to aid 
in the answering of questions related to this data. Additionally,
data mining is capable of assigning new meaning to data, thus 
creating information that was not originally present or obvious. 
While data mining does not solve problems in and of itself, it 
does answer questions about the data. These answers can lead to 
more data mining problems, and occasionally to a solution to the 
original problem. Data mining is done in nine general steps. The 
steps are: problem statement, get data, enrich data, clean data, 
integrate data, transform data, mine, validate or interpret 
results, and make the results actionable. These will be discussed 
in greater depth below.

Two kinds of problems that often arise in data mining are clustering
and classification. Clustering involves grouping objects
based on the similarities of their attributes. This can be useful
for applications such as image processing or online shopping
recommendations. On the other hand, classification is assigning
one of a set of definite labels to an object. Clustering is
an example of unsupervised learning, while classification is
an example of supervised learning. Many approaches to data
mining use a loss function to make their predictions. A loss
function is a mapping from some event to a real number based
on the cost of that event happening. This cost does not have
to be strictly monetary; it could be time, lives, or infinite other
types of costs. It is important to note that, even once data
mining on a given project is complete, it does not definitively
tell the data miner what the next steps are. To obtain that
information, it is necessary to analyze the results of the data
mining.

There are ten commonly used algorithms in data mining.
These are:

1. C4.5  This algorithm is a descendant of the older ID3 and CLS algorithms.
Like these, it uses decision trees to classify data. The
tree is grown in a divide-and-conquer manner by recursively
partitioning the data and then assigning a leaf to a class label
once the data cannot be partitioned or subsequent partitions
will not aid in the prediction. This algorithm can handle either
numeric values or nominal values, and the nature of the data
affects how the partitioning is done. Once this is complete,
the tree is pruned to prevent overfitting. Unfortunately, C4.5’s
generation of rulesets is very CPU and memory intensive,
taking 9715 seconds to analyze 100,000 samples.

2. K-means  This algorithm partitions data into a given number of clusters,
k. First, k centroids are chosen, and there are many different
ways to pick these centroids. One commonly used approach
is randomly sampling the data. Once this is complete, each
data point is assigned to the nearest centroid. Obviously, there
is no single way to decide ”nearness”; as such, the decision
of how to compute distance is extremely important for the
success of this algorithm. Next, the mean of every cluster
is computed, and this mean takes the place of the centroid.
The process is complete when the data assigned to a cluster
no longer changes. Unfortunately, there are numerous issues
with this algorithm. The most obvious of these is that setting
k can be difficult, and setting k poorly will dramatically affect
the algorithm’s performance. Additionally, it is sensitive to
outliers.

3. Support vector machines (SVM)  SVM are a robust and effective 
approach to classification.
SVM are best understood from the standpoint of a linearly
separable data set. In this case, the SVM finds a hyperplane
separating the data into two classes by maximizing the distance
from the line to the nearest points from either class. Of
course, data is almost never linearly separable, so part of the
SVM formulation is handling the case where some data is on
the wrong side of the line. Additionally, a kernel function
can be applied when the data is not linearly separable, which
is currently an area of active research. SVM struggled with
with slow performance, but this is also being actively solved.
This is done mostly by breaking the large optimization problem
into many smaller problems.

4. Apriori  This algorithm is intended to find frequent itemsets - that is
items that occur together frequently - through generating candidates.
First, it generates candidates for frequent itemsets of
a certain size k+1 from the itemsets of size k. Then it calculates
the support for each candidate to determine which of the
candidates is an actual frequent itemset. The candidates that
meet the minimum support are added to the frequent itemsets
of size k+1, and the process is repeated. Unfortunately, the
Apriori algorithm generates a large number of candidate sets
in some cases, namely when there are many frequent itemsets
or low minimum support. Additionally, the algorithm
must generate many candidates for itemsets with a large size.
For example 2100 candidates must be generated for frequent
itemsets of size 100.

5. Expectation-Maximization (EM)  The EM algorithm involves 
fitting normal mixture models
by maximum likelihood. This has numerous advantages, one
of which is that it allows the algorithm to model continuous
data well. It also allows for the modeling of latent variables,
or variables that aren’t explicit in the dataset. The algorithm
proceeds as one would expect: by finding expectation of the
log likelihood function of the conditional distribution of a
latent variable given the data and a current estimate of the
parameters of the distribution. Then it finds the parameter that
maximizes this value. The issue with this algorithm is that the
number of clusters must be chosen, and there are many ways
to accomplish that.

6. PageRank  Famously developed by Larry Page and Sergey Brin and used
by Google, PageRank ranks web pages by considering a link
to a page as a vote for that page. Then it ranks the pages
by how many ”votes” they get. However, the algorithm also
considers the importance of a page doing the voting, since
a higher-ranked page should have more clout in what pages
it deems important. It accomplishes this task by treating the
web as a directed graph and then counting inlinks as part of
a vertex’s indegree and the outlinks as a vertex’s outdegree.
The algorithm then simply iterates through the vertices until
the ranks do not change very much.

7. AdaBoost  AdaBoost is an ensemble learning system developed by Yoav
Freund and Robert Schapire. It is extremely accurate and
simple, with implementation possible in just ten lines of code.
First, this algorithm uses a simple learning algorithm to generate
class labels for the data. Then it uses the training data to
test this learner. It repeats this process T times, and then the
result is decided by weighted majority voting by the results of
the weak learners. Of course, this does not handle the multiclass
case. There is a variant of the algorithm for this case
that splits the decisions into a series of binary classification
problems. It is also interesting to note that this series of weak
learner is comparable in accuracy to a strong learner.

8. K-nearest neighbors  This algorithm involves finding k items 
in the data set that
are closest to a given object. Like k-means, this ”closeness”
is not defined, so it is necessary to choose a distance metric
carefully. For an unlabeled object, the object’s k nearest
neighbors are calculated, then those neighbors’ class labels
vote on the class of the object. One issue with this algorithm
is that it is very sensitive to the choice of k. Also, the majority
vote is not always a good way to decide the class of an object,
since some neighbors may be vastly closer than others. To
solve this, a distance-weighted voting system can be used.
K-nearest neighbors is well-suited for data in which an object
can have multiple labels.

9. Naive Bayes  Naive Bayes is a commonly used algorithm that, while usually
outperformed by more complex algorithms, can be relied
upon to provide good results in a wide variety of problems.
Naive Bayes simply calculates the conditional probability of
the features given the label and chooses the label with the
maximum probability. However, this means that the data must
be discretized in some way to ensure that the probability can
be computed. Additionally, this algorithm assumes that the
features are all independent. They rarely are independent, but
this does not make the algorithm perform poorly in practice.

10. CART  CART, standing for Classification and Regression Trees, was
developed by Leo Breiman, Jerome Friedman, Richard Olshen,
and Charles Stone. The algorithm builds a tree to maximal
size and then prunes it back recursively based on which
subtree contributes the least to the accuracy of the classification.
This accuracy is determined based on external test data or
cross validation. CART also supports class balancing, which
is an endeavor to make sure the pruning does not unfairly
take more occurrences of one class away. The importance of
an attribute is calculated by the sum of the improvement in
performance in all nodes where the attribute is split upon.

Big Data Technology

1. REST Representational State Transfer
REpresentational State Transfer (REST) is an architectural style 
that defines a set of constraints and attributes based on HTTP. 
Web services that conform to REST-style or REST-style Web 
services provide interoperability between computer systems on the 
Internet. RESTful web services allow the requesting system to 
access and manipulate the textual representation of web resources 
by using a uniform and predefined set of stateless operations. 
Other types of web services (such as SOAP web services) expose 
their own set of arbitrary operations.

The web resource is first defined on the World Wide Web as a 
document or file specified by its URL. However, today they have a 
more general and abstract definition, including anything that can 
be identified, named, processed, or processed in any way on the 
Internet. In a RESTful web service, a request for a resource URI 
triggers a response that may be in XML, HTML, JSON, or other 
format. The response can confirm that some changes have been made 
to the stored resources, and the response can provide hypertext 
links to other related resources or collections of resources. 
When using HTTP, the most common operations are GET, POST, PUT, 
DELETE, and other predefined CRUD HTTP methods.

By using stateless protocols and standard operations, REST 
systems are designed to achieve fast performance, reliability, 
and growth capabilities, even at runtime, by reusing manageable 
and updated components without affecting the entire system.

The transformation of representative countries was introduced and 
defined in Roy Fielding's doctoral dissertation in 2000. 
Fielding's paper explains the REST principles known as the HTTP 
Object Model since 1994 and is used to design the HTTP 1.1 and 
Uniform Resource Identifier (URI) standards. The term is intended 
to evoke a behavioral image of a well-designed web application: it
 is a web resource network (virtual state machine) where the user 
selects the link (eg / user / tom) in the application The progress
 of the program, as well as operations such as GET or DELETE 
(state transitions), cause the next resource (indicating the next 
state of the application) to be passed to the user for its use.

2. Swagger
Swagger is an open source software framework powered by a large 
ecosystem of tools that helps developers design, build, document, 
and use RESTful Web services. Although most users recognize Swagger
 through the Swagger UI tool, the Swagger toolset includes support
 for automatic documentation, code generation, and test case 
generation.

3. Docker
Docker is a computer program that performs operating system level 
virtualization, also known as containerization. It was developed by
 Docker, Inc. Docker is primarily developed for Linux. It uses 
Linux kernel resource isolation features (such as cgroups and 
kernel namespaces) and support for federated file systems (such as
 OverlayFS, etc.) to allow independent containers to run in a 
single Linux instance, thus avoiding startup. And keep the overhead
 of the virtual machine (VM). The Linux kernel's support for 
namespaces often isolates the application's view of the operating 
environment, including the process tree, network, user IDs, and 
mounted file systems, while the kernel's cgroups provide memory and
 CPU resource limits. Beginning with version 0.9, Docker uses the 
libcontainer library as its own way to directly use the 
virtualization tools provided by the Linux kernel, in addition to 
using libvirt, LXC, and systemd-nspawn to use the abstract 
virtualization interface.

\begin{acks}

  The authors would like to thank Dr.~Gregor~von~Laszewski for his
  support and suggestions to write this paper.

\end{acks}


\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

